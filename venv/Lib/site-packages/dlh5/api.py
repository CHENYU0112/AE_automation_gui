from __future__ import annotations

import contextlib
import copy
import io
import json
import logging
import os
import platform
import sys
import traceback
import uuid
import zipfile
from collections import namedtuple
from datetime import datetime, timezone
from getpass import getuser
from numbers import Number
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable, Literal

import numpy as np
from h5py import Dataset, ExternalLink, Group
from h5py import File as h5pyFILE
from packaging.version import Version
from wavewatson import Waveform

from . import __version__
from .errors import (
    AccessPermissionError,
    BrokenExternalLinkError,
    ChannelNotFoundError,
    CompatibilityError,
    DatasetExistsError,
    InvalidIndexError,
    MetadataExistsError,
    UnsupportedDataTypeError,
)
from .filelock import RecoverableSoftFileLock
from .index import Index
from .mocks import DummyLock
from .util import (
    check_for_nextcloud_monitoring,
    compress,
    exists_encoding,
    json_default_serializer,
    node_get_attribute,
    node_set_attribute,
    to_safe_key,
    uncompress,
)

if TYPE_CHECKING:
    from collections.abc import Iterable, Iterator


class _CONSTANTS:
    class DTypes:
        Unknown = "__unknown__"
        String = "string"
        Number = "number"
        ComplexNumber = "complex_number"
        Waveform = "waveform"
        NumericArray = "numeric_array"
        ComplexNumericArray = "complex_numeric_array"
        File = "file"

    class H5Groups:
        Data = "data"  # /data
        Static = "static"  # /static
        Results = "results"  # /data/results
        OpCond = "opcond"  # /data/opcond

    class Indexes:
        OperatingConditions = "index_operating_conditions"
        Metadata = "index_metadata"
        Results = "index_results"
        ResultsMetadata = "index_results_metadata"

    class Storage:
        MaxChunkSize = 10 * 1024**2


_C = _CONSTANTS
FileName = str


class DLH5:
    """"""

    _LIB_VERSION = Version(__version__)
    LIB_VERSION = f"{_LIB_VERSION.major}.{_LIB_VERSION.minor}.{_LIB_VERSION.micro}"

    class Mode:
        Write = "w"
        Append = "a"
        ReadWrite = "r+"
        ReadOnly = "r"

    def __init__(
        self,
        file: Path | (str | io.IOBase),
        mode: Literal["w", "a", "r+", "r"] | str = Mode.ReadOnly,
        auto_create_index=True,
        logger: bool | (logging.Logger | (str | Path)) | None = None,
        lock_file_tmo: int = -1,
        zip: bool = False,
        zip_inplace: bool = False,
        unzip_inplace: bool = False,
        zip_opts: bool | int | dict | None = None,
        entrypoint: str = "/",
        **kwargs,
    ):
        """
        Opens and maybe initializes a DLH5 file

        :param file: The desired file path or file-like object (io.IOBase)
        :param mode: 'w' for write, 'a' for append, 'r' for read-only, 'r+' for read-write
        :param timeout_s: The timeout in seconds to wait until the file can be opened (based on a soft file lock)
        :param auto_create_index: If True the index is regenerated upon closing the file, given that correlating data
                                  has changed
        :param logger: If True, the lib will log via stdout.
                       If a string is given, the library will log to an existing logger with this name.
                       If a pathlib.Path instance is given, the lib will log to stdout and to the file path given.
                       If a logging.Logger instance is given the lib will use it for logging.
                       If None is given, a default logger named "DLH5" will be created, using a default NullHandler.
        :param lock_file_tmo: If a positive int or 0 is given, a soft file lock will ensure that multiple
                              processes/threads cannot work on the same file at the same time.
                              The argument represents the timeout in seconds the process
                              waits for acquiring the file lock at most before raising an exception.
                              Negative values disable the file lock mechanism.
                              A timeout of 0 means, that there is exactly one attempt to acquire the file lock.
        :param zip: If True, compress the file after closing (only when in write mode).
        :param zip_inplace: After compression, replace original file by the compressed version.
        :param unzip_inplace: After decompression, replace compressed file by the uncompressed version.
        :param zip_opts: Keyword arguments that are passed to :class:`zipfile.ZipFile` when compressing the file.
        :param entrypoint: Path to HDF5 group (e.g. ``/subgroup/subsubgroup``) with the DLH5 root (root by default)
        :key compression_opts: A dictionary holding compression options for creation of compressed datasets.
                               See doc of `h5py.Group.create_dataset
                               <https://docs.h5py.org/en/stable/high/group.html#h5py.Group.create_dataset>`_
                               for more details.
        :key debug: If True the logging level will be set to DEBUG.
        :key log_prefix: The prefix for all log messages
        """
        self._fh: h5pyFILE | None = None

        self._entrypoint = "/" + entrypoint.strip("/")
        self._zip_enabled = zip
        self._zip_inplace = zip_inplace
        self._unzip_inplace = unzip_inplace
        if zip_opts is not None and not isinstance(zip_opts, (int, dict)):
            msg = "'zip_opts' must be either an integer or a dict!"
            raise TypeError(msg)
        self._zip_opts = zip_opts

        # shuffle is turned off because it severely limits the achievable compression rates
        self._comp_opts = kwargs.get("compression_opts", {"compression": "gzip", "shuffle": False})
        if mode not in ("w", "a", "r", "r+"):
            msg = "mode must be one of following values: 'w', 'a', 'r', 'r+'!"
            raise ValueError(msg)
        self._mode = mode
        self._auto_create_index = bool(auto_create_index)
        if isinstance(file, (Path, str)):
            self._fileio = Path(file).absolute()
            self._fileio.parent.mkdir(parents=True, exist_ok=True)
        elif isinstance(file, io.IOBase):
            self._fileio = file
            lock_file_tmo = -1
        else:
            msg = f"Cannot open {file!r}!"
            raise TypeError(msg)

        # A soft-file lock based on a *.lock file. It is self-recoverable in case the application crashes and
        # leaves behind a lock file
        if not isinstance(lock_file_tmo, int):
            msg = "Argument lock_file_tmo is not of type integer!"
            raise TypeError(msg)
        if lock_file_tmo >= 0:
            self._lock = RecoverableSoftFileLock(str(self._fileio) + ".lock")
        else:
            self._lock = DummyLock()  # type: ignore
        self._lock_tmo = lock_file_tmo

        # Signals if the index has to be recreated (is auto-index is enabled) once the file is closed
        self._index_out_of_date = False

        self._debug = bool(kwargs.get("debug", False))
        loglevel = [logging.INFO, logging.DEBUG][self._debug]
        FMT = logging.Formatter("%(levelname)-7s|%(asctime)s.%(msecs)03d|  %(message)s", datefmt="%H:%M:%S")
        if isinstance(logger, bool):
            log = logging.getLogger(__name__)
            log.handlers.clear()
            log.setLevel(loglevel)

            if logger:
                sh_name = self.__class__.__name__ + "_StreamHandler"
                if not any(h.name == sh_name for h in log.handlers):
                    sh = logging.StreamHandler(sys.stdout)
                    sh.name = sh_name
                    sh.setLevel(loglevel)
                    sh.setFormatter(FMT)
                    log.addHandler(sh)
            else:
                log.addHandler(logging.NullHandler())

        elif isinstance(logger, Path):
            log = logging.getLogger(__name__)
            log.handlers.clear()
            log.setLevel(loglevel)

            sh_name = self.__class__.__name__ + "_StreamHandler"
            if not any(h.name == sh_name for h in log.handlers):
                sh = logging.StreamHandler(sys.stdout)
                sh.setLevel(loglevel)
                sh.setFormatter(FMT)
                log.addHandler(sh)

            logger.parent.mkdir(parents=True, exist_ok=True)
            fh_name = self.__class__.__name__ + "_FileHandler"
            if not any(h.name == fh_name for h in log.handlers):
                fh = logging.FileHandler(str(logger), mode="w")
                fh.name = fh_name
                fh.setLevel(loglevel)
                fh.setFormatter(FMT)
                log.addHandler(fh)

        elif isinstance(logger, logging.Logger):
            log = logger

        elif isinstance(logger, str):
            log = logging.getLogger(logger)

        else:
            log = logging.getLogger(__name__)
            log.setLevel(loglevel)

        class PrefixLoggingAdapter(logging.LoggerAdapter):
            def process(self, msg, kwargs):
                return f"{self.extra.get('prefix', '')}{msg}", kwargs

        self.log = PrefixLoggingAdapter(log, {"prefix": kwargs.get("log_prefix", self.__class__.__name__ + " | ")})

        self.open()
        self.__check_version_compatibility()

    def __del__(self):
        if self._fh is not None and isinstance(self._fh, h5pyFILE):
            try:
                self._fh.flush()
                self._fh.close()
                self._fh = None
                self._lock.release()
                DLH5._cleanup_log_handler()
            except Exception:
                pass

    @staticmethod
    def _cleanup_log_handler():
        """
        Closes each log handler separately, when the module logger was used.
        """
        log = logging.getLogger(__name__)
        for handler in log.handlers:
            handler.flush()
            handler.close()

    def _set_index_out_of_date(self):
        self._index_out_of_date = True

    def __requires_write_permissions(self):
        if self._mode == self.Mode.ReadOnly:
            msg = "Cannot set attribute when file is opened in read-only mode!"
            raise AccessPermissionError(msg)

    def __check_version_compatibility(self):
        file_version = self.get_version_tuple()
        if self._LIB_VERSION.major < file_version.major:
            msg = (
                f"The major version {self._LIB_VERSION.major} of the DLH5py library is older then the "
                f"major version that was used to create the file, which is {file_version.major}! "
                f"Please upgrade your dlh5-py package to at least {file_version.major}.0.0!"
            )
            raise CompatibilityError(msg)

    @property  # type: ignore
    def file(self) -> h5pyFILE:
        """
        Returns the h5py file descriptor
        """
        return self._fh

    def _rel_to_root(self, name: str) -> Group:
        """Return (possibly create) a H5 group at the DLH5 root (optionally not HDF5 file root)"""
        if not name:
            return self.file[self._entrypoint]
        path = self._entrypoint + "/" + name.strip("/")

        if self._mode == self.Mode.ReadOnly:
            return self.file[path]
        return self.file.require_group(path)

    @property  # type: ignore
    def root_grp(self) -> Group:
        """
        Returns the H5 group that holds the DLH5 root group
        """
        return self._rel_to_root("")

    @property  # type: ignore
    def data_grp(self) -> Group:
        """
        Returns the H5 group that holds iteration result groups with the naming scheme 00000000 ... 99999999
        """
        return self._rel_to_root(_C.H5Groups.Data)

    @property  # type: ignore
    def static_grp(self) -> Group:
        """
        Returns the H5 group that holds static test information like logs and test interface
        """
        return self._rel_to_root(_C.H5Groups.Static)

    @property
    def fileio(self) -> Path | io.IOBase:
        """
        Returns the file path to the result file
        """
        return self._fileio

    def __enter__(self):
        """
        A context manager ENTER that will open the file
        """
        self.open()
        return self

    def __exit__(self, type, value, tb):
        """
        A context manager EXIT that will close the file
        """
        self.close()

    def open(self) -> bool:
        """
        Opens the configured file in a specific mode.
        Opening the file will create a soft file lock (.lock file) to prevent other processes from opening the file.

        :returns: Boolean if the file was opened by this function (True) or already open before (False)
        """
        if isinstance(self._fh, h5pyFILE):
            return False

        self._lock.acquire(timeout=self._lock_tmo)

        if isinstance(self.fileio, io.IOBase):
            new = self._mode == self.Mode.Write
            self._fh = h5pyFILE(self.fileio, self._mode)
        else:
            new = not self.fileio.exists() or self._mode == self.Mode.Write

            if zipfile.is_zipfile(self.fileio):
                self._fileio = uncompress(self.fileio, inplace=self._unzip_inplace)

            self._fh = h5pyFILE(str(self.fileio), self._mode)

        if self._mode == self.Mode.ReadOnly and self._entrypoint not in self.file:
            raise KeyError(self._entrypoint, "Specified HDF5 group not found") from None
        self.file.require_group(self._entrypoint)

        self._index_out_of_date = False

        now_utc = datetime.now(tz=timezone.utc).isoformat()
        if new:
            # ensure the is no potential interference from file sync clients
            # only applicable to objects referring to files on disk
            if isinstance(self.fileio, (io.FileIO, Path)):
                check_for_nextcloud_monitoring(self.fileio)

            # Only add those attributes if the file is created
            self.file_add_metadata("dlh5_version", self.LIB_VERSION)
            self.file_add_metadata("uuid", str(uuid.uuid4()))
            self.file_add_metadata("creation_date", now_utc)
            self.file_add_metadata("creation_date_utc", now_utc)
            self.file_add_metadata("user_name", getuser())
            self.file_add_metadata("host", platform.node())
            self.file_add_metadata("python_version", platform.python_version())
            self.file_add_metadata("os", platform.platform())
            self.file_add_metadata("bits", platform.architecture()[0])
            self.file_add_metadata("closed_by_destructor", 0)

        if self._mode != self.Mode.ReadOnly:
            self.file_add_metadata("modified_date", now_utc)
            self.file_add_metadata("modified_date_utc", now_utc)

        if "dlh5_version" not in self.root_grp.attrs:
            msg = "This is not a DLH5 file!"
            raise RuntimeError(msg)

        # Create groups if not existent
        _ = self.data_grp
        _ = self.static_grp

        return True

    def close(self, skip_index=False):
        """
        Close and unlock the file and maybe recreate the index.

        :param skip_index: If True, index generation is skipped.
        """

        if isinstance(self._fh, h5pyFILE):
            if self._mode != self.Mode.ReadOnly:
                self.file_add_metadata("modified_date", datetime.now(tz=timezone.utc).isoformat())
                self.file_add_metadata("modified_date_utc", datetime.now(tz=timezone.utc).isoformat())
            try:
                if self._auto_create_index and self._index_out_of_date and not skip_index:
                    self.create_indexes()
            except Exception:  # pragma: no cover
                self.log.error(f"Could not create index: {traceback.format_exc()}")
                raise
            finally:
                self._fh.flush()
                self._fh.close()
                self._fh = None
        self._lock.release()

        if self._zip_enabled and isinstance(self.fileio, (str, Path)) and self._mode != self.Mode.ReadOnly:
            compress(self.fileio, inplace=self._zip_inplace, opts=self._zip_opts)

    def create_indexes(self):
        """
        Creates Pandas HDFStore tables for all available group parameters, group metadata and scalar group results
        """

        parameters = []
        metadata = []
        results = []
        results_metadata = []

        grp_ids = sorted(map(int, self.data_grp.keys()))
        self.log.info("Reading data for index...")
        for gid in grp_ids:
            grp = self.file_require_group(gid)

            parameters.append((int(gid), {k: v[0] for k, v in self.group_get_operating_conditions(grp, None).items()}))
            metadata.append((int(gid), self.group_get_metadata(grp, None)))
            res = {}
            res_meta = {}
            for ch in self.list_numeric_channels(grp) + self.list_string_channels(grp):
                data, meta = self.group_get_channel(grp, ch)
                res[ch] = data
                for k, v in meta.items():
                    res_meta[ch + "_" + str(k)] = v
            results.append((int(gid), res))
            results_metadata.append((int(gid), res_meta))

        self.log.info("Writing index for operating conditions...")
        self.static_add_text_data(
            _C.Indexes.OperatingConditions, json.dumps(parameters, default=json_default_serializer)
        )
        self.log.info("Writing index for metadata...")
        self.static_add_text_data(_C.Indexes.Metadata, json.dumps(metadata, default=json_default_serializer))
        self.log.info("Writing index for results...")
        self.static_add_text_data(_C.Indexes.Results, json.dumps(results, default=json_default_serializer))
        self.log.info("Writing index for result metadata...")
        self.static_add_text_data(
            _C.Indexes.ResultsMetadata, json.dumps(results_metadata, default=json_default_serializer)
        )
        self.log.info("Writing index done.")
        self._index_out_of_date = False

    def _check_index_out_of_date(self):
        """
        Checks if the index has been created yet or is out of date and throws an InvalidIndexError exception in case.

        :raises: InvalidIndexError
        """
        if self._index_out_of_date:
            msg = "Index is out of date. Use 'create_index' first!"
            raise InvalidIndexError(msg)

    def get_version(self) -> str:
        """
        Returns the DLH5 version information of the stored file
        """
        _v = self.get_version_tuple()
        return f"{_v.major}.{_v.minor}.{_v.micro}"

    def get_version_tuple(self) -> Version:
        """
        Returns the DLH5 version information of the stored file
        """
        return Version(self.file_get_metadata("dlh5_version"))

    def file_get_group(self, gid: int | Group) -> Group:
        """
        Returns the group that holds iteration results.
        If the required group does not yet exist an exception will be raised.

        :param gid: The group number or directly the respective group object.
        :raises LookupError: If group does not yet exist
        """

        if isinstance(gid, int):
            if gid < 0:
                keys = list(self.data_grp.keys())
                gid = max(map(int, keys)) + 1 + gid if len(keys) > 0 else 0
            grpname = _gid2str(gid)
            if grpname in self.data_grp:
                return self.data_grp[grpname]

            msg = f"Group with gid {grpname} does not exist!"
            raise LookupError(msg)
        if isinstance(gid, Group):
            if gid.name is None:
                msg = f"Group with gid {gid} does not exist!"
                raise LookupError(msg)
            if gid.name in self.root_grp:
                return gid
            msg = f"Group {gid.name} not in file!"
            raise LookupError(msg)

        msg = f"Wrong input type {type(gid)} of argument 'gid'!"
        raise TypeError(msg)

    def file_require_group(self, gid: int | Group) -> Group:
        """
        Returns the group that holds iteration results. If the required group does not yet exist it will be created.

        :param gid: The group number or directly the respective group object.
        """
        if self._mode == self.Mode.ReadOnly:
            return self.file_get_group(gid)

        if isinstance(gid, int):
            if gid < 0:
                msg = "If gid is an integer it must be >= 0!"
                raise IndexError(msg)
            return self.data_grp.require_group(_gid2str(gid))
        if isinstance(gid, Group):
            return gid

        msg = f"Wrong input type {type(gid)} of argument 'gid'!"
        raise TypeError(msg)

    def _group_get_node_results(self, gid: int | Group) -> Group:
        """
        Returns the results subgroup of the group that holds iteration results.
        If the required group does not yet exist an Exception is raises.

        :param gid: The group number or directly the respective group object.
        """
        if self._mode == self.Mode.ReadOnly:
            return self.file_get_group(gid).get(_C.H5Groups.Results)

        return self.file_require_group(gid).require_group(_C.H5Groups.Results)

    def _group_require_node_results(self, gid: int | Group) -> Group:
        """
        Returns the results subgroup of the group that holds iteration results.
        If the required group does not yet exist it will be created.

        :param gid: The group number or directly the respective group object.
        """
        if self._mode == self.Mode.ReadOnly:
            return self._group_get_node_results(gid)

        return self.file_require_group(gid).require_group(_C.H5Groups.Results)

    def _group_get_node_opcond(self, gid: int | Group) -> Group:
        """
        Returns the results subgroup of the group that holds iteration results.
        If the required group does not yet exist an Exception is raises.

        :param gid: The group number or directly the respective group object.
        """
        if self._mode == self.Mode.ReadOnly:
            return self.file_get_group(gid).get(_C.H5Groups.OpCond)

        return self.file_require_group(gid).require_group(_C.H5Groups.OpCond)

    def _group_require_node_opcond(self, gid: int | Group) -> Group:
        """
        Returns the results subgroup of the group that holds iteration results.
        If the required group does not yet exist it will be created.

        :param gid: The group number or directly the respective group object.
        """
        if self._mode == self.Mode.ReadOnly:
            return self._group_get_node_opcond(gid)

        return self.file_require_group(gid).require_group(_C.H5Groups.OpCond)

    def file_delete_group(self, gid: int | Group):
        """
        Deletes a group that holds iteration results

        :param gid: The group number or directly the respective group object.
        """
        self._set_index_out_of_date()
        grp = self.file_require_group(gid)
        del self._fh[grp.name]

    def file_list_groups(self) -> list[int]:
        """
        Lists all groups
        """
        return list(map(int, self.data_grp.keys()))

    def file_find_groups_by_opcond(self, opcond: dict[str, Any]) -> list[int]:
        """
        Finds groups by operating conditions.

        .. code-block:: python

            file.file_find_groups_by_opcond({"paramA": 1, "paramB": 2})

        :param opcond: The operating conditions to search for
        """
        self._check_index_out_of_date()
        oc = self.get_index_operating_conditions().as_dataframe()
        for k, v in opcond.items():
            try:
                oc = oc[oc[k] == v]
            except KeyError:
                msg = f"Operating condition {k!r} not found in index!"
                raise KeyError(msg) from None
        return oc.index.tolist()

    def file_find_groups_by_metadata(self, metadata: dict[str, Any]) -> list[int]:
        """
        Finds groups by metadata.

        .. code-block:: python

            file.file_find_groups_by_metadata({"metaA": 1})

        :param meta: The metadata to search for
        """
        self._check_index_out_of_date()
        md = self.get_index_metadata().as_dataframe()
        for k, v in metadata.items():
            try:
                md = md[md[k] == v]
            except KeyError:
                msg = f"Metadata {k!r} not found in index!"
                raise KeyError(msg) from None
        return md.index.tolist()

    def file_iter_groups(self) -> Iterator[Group]:
        """
        Returns an iterator over all groups
        """
        for gid in self.file_list_groups():
            yield self.file_get_group(gid)

    def file_add_metadata(self, key: str, value: Any, *, overwrite: bool = True):
        """
        Adds metadata at file level

        :param key: The metadata name
        :param value: The metadata value
        :param overwrite: If the key already exists, set this argument to overwrite the data
        """
        node_set_attribute(self.root_grp, key, value, overwrite)

    def file_get_metadata(self, key: str | None = None) -> Any | dict[str, Any]:
        """
        Returns metadata from file level

        :param key: The metadata name
        """
        return node_get_attribute(self.root_grp, key)

    def group_add_metadata(
        self,
        gid: int | Group,
        key: str,
        value: Any,
        overwrite: bool = True,
    ):
        """
        Adds metadata at group/iteration result level

        :param gid: The group/iteration number or directly the respective group object.
        :param key: The metadata name
        :param value: The metadata value
        :param overwrite: If the key already exists, set this argument to overwrite the data
        """
        self._set_index_out_of_date()
        grp = self.file_require_group(gid)
        node_set_attribute(grp, key, value, overwrite)

    def group_add_operating_condition(
        self,
        gid: int | Group,
        key: str,
        value: Any,
        unit: str = "",
        overwrite: bool = True,
    ):
        """
        Adds operating conditions at group/iteration result level

        :param gid: The group/iteration number or directly the respective group object.
        :param key: The name of the operating condition
        :param value: The value
        :param unit: The unit
        :param overwrite: If the key already exists, set this argument to overwrite the data
        """
        self._set_index_out_of_date()
        grp = self._group_require_node_opcond(gid)

        if isinstance(value, str):
            return self.__add_string_channel(grp, key, value, "utf-8", meta_data={"unit": unit}, overwrite=overwrite)
        if isinstance(value, Number):
            return self.__add_numeric_channel(grp, key, value, unit, overwrite=overwrite)

        msg = "Only string and numeric types are allowed for operating conditions!"
        raise UnsupportedDataTypeError(msg)

    def group_get_metadata(self, gid: int | Group, key: str | None = None) -> Any:
        """
        Returns metadata at group/iteration result level

        :param gid: The group/iteration number or directly the respective group object.
        :param key: The metadata name. If omitted all metadata are returned
        """
        if not isinstance(key, str) and key is not None:
            msg = "Argument 'key' must be a string!"
            raise TypeError(msg)
        grp = self.file_get_group(gid)
        return node_get_attribute(grp, key)

    def group_get_operating_conditions(
        self, gid: int | Group, key: str | None = None
    ) -> tuple[Any, str] | dict[str, tuple[Any, str]]:
        """
        Returns all metadata (parameters and real metadata) at group/iteration result level

        :param gid: The group/iteration number or directly the respective group object.
        :param key: The operating cond name. If omitted all are returned
        """
        grp = self._group_get_node_opcond(gid)
        if key is None:
            oc = {}
            for ch in grp:
                data, meta = self.__get_channel(grp, ch)
                oc[ch] = (data, meta.get("unit", ""))
            return oc

        data, meta = self.__get_channel(grp, key)
        return data, meta.get("unit", "")

    def channel_add_metadata(
        self,
        gid: int | Group,
        cname: str,
        key: str,
        value: Any,
        overwrite: bool = True,
    ):
        """
        Adds metadata at channel/dataset level. The channel/dataset has to be created before using 'channel_add_data'

        :param gid: The group/iteration number or directly the respective group object.
        :param cname: The channel/dataset name
        :param key: The metadata name
        :param value: The metadata value
        :param overwrite: If the key already exists, set this argument to overwrite the data
        """
        grp = self._group_require_node_results(gid)
        if cname in grp:
            node_set_attribute(grp.get(cname), key, value, overwrite)
        else:
            msg = f"Channel {cname!r} does not exist yet. First create a channel using 'channel_add_data'!"
            raise ChannelNotFoundError(msg)

    def group_get_channel_metadata(self, gid: int | Group, cname: str, key: str | None = None) -> Any:
        """
        Returns metadata at channel/dataset level

        :param gid: The group/iteration number or directly the respective group object.
        :param cname: The channel/dataset name
        :param key: The metadata name
        """
        grp = self._group_get_node_results(gid)
        if cname in grp:
            return node_get_attribute(grp.get(cname), key)

        msg = f"Channel {cname!r} does not exist yet. First create a channel using 'channel_add_data'!"
        raise ChannelNotFoundError(msg)

    def group_add_numeric_channel(
        self,
        gid: int | Group,
        cname: str,
        data: Number,
        unit: str | None = "",
        *,
        meta_data: dict[str, Any] | None = None,
        overwrite: bool = True,
        **kwargs,
    ) -> Dataset:
        """
        Adds a new numeric channel.

        :param gid: The group/iteration number or directly the respective group object.
        :param cname: The channel/dataset name
        :param data: The result data to store
        :param unit: The unit of the result
        :param meta_data: A dictionary of optional metadata to store at channel/dataset level.
                          See also 'channel_add_metadata'.
        :param overwrite: If the channel already exists, set this argument to overwrite the data
        """
        self._set_index_out_of_date()
        grp = self._group_require_node_results(gid)
        return self.__add_numeric_channel(grp, cname, data, unit, meta_data=meta_data, overwrite=overwrite)

    def __add_numeric_channel(
        self,
        grp: Group,
        cname: str,
        data: Number,
        unit: str | None = "",
        *,
        meta_data: dict[str, Any] | None = None,
        overwrite: bool = True,
        **kwargs,
    ) -> Dataset:
        """
        Adds a new numeric channel.

        :param grp: The respective group object to store the dataset.
        :param cname: The channel/dataset name
        :param data: The result data to store
        :param unit: The unit of the result
        :param meta_data: A dictionary of optional metadata to store at channel/dataset level.
                          See also 'channel_add_metadata'.
        :param overwrite: If the channel already exists, set this argument to overwrite the data
        """
        self._set_index_out_of_date()
        meta_data = copy.deepcopy(meta_data) or {}
        cname = to_safe_key(cname)

        if cname in grp:
            if overwrite:
                self.log.debug(f"Overwriting dataset {cname}...")
                del grp[cname]
            else:
                msg = f"Dataset {cname} already exists!"
                raise DatasetExistsError(msg)

        complex_ = False
        if isinstance(data, int):
            data = np.int64(data)
        elif isinstance(data, float):
            data = np.float64(data)
        elif isinstance(data, (complex, np.complex_)):
            data = np.complex128(data)
            complex_ = True
        elif not isinstance(data, np.number):
            msg = "Argument 'data' only accepts Python int/float or scalar numpy types!"
            raise TypeError(msg)

        if complex_:
            ds = grp.create_dataset(
                cname,
                data=np.array([[data.real], [data.imag]]),
            )
            meta_data["dtype"] = _C.DTypes.ComplexNumber
        else:
            ds = grp.create_dataset(
                cname,
                shape=(1,),
                data=np.array([data]),
            )
            meta_data["dtype"] = _C.DTypes.Number

        meta_data["name"] = str(cname)
        meta_data["unit"] = str(unit)
        for k, v in meta_data.items():
            node_set_attribute(ds, k, v)
        return ds

    def group_add_string_channel(
        self,
        gid: int | Group,
        cname: str,
        data: str | bytes,
        encoding: str | None = None,
        *,
        meta_data: dict[str, Any] | None = None,
        overwrite: bool = True,
        **kwargs,
    ) -> Dataset:
        """
        Adds a new string channel.

        :param gid: The group/iteration number or directly the respective group object.
        :param cname: The channel/dataset name
        :param data: The result data to store
        :param encoding: The encoding to use when storing the string.
                         If data is a bytes object, the encoding can be dismissed, which
                         enforces the string data to also be read as bytes
        :param meta_data: A dictionary of optional metadata to store at channel/dataset level.
                          See also 'channel_add_metadata'.
        :param overwrite: If the channel already exists, set this argument to overwrite the data
        """
        self._set_index_out_of_date()
        grp = self._group_require_node_results(gid)
        return self.__add_string_channel(grp, cname, data, encoding, meta_data=meta_data, overwrite=overwrite)

    def __add_string_channel(
        self,
        grp: Group,
        cname: str,
        data: str | bytes,
        encoding: str | None = None,
        *,
        meta_data: dict[str, Any] | None = None,
        overwrite: bool = True,
    ) -> Dataset:
        """
        Adds a new string channel.

        :param grp: The respective group object to store the dataset.
        :param cname: The channel/dataset name
        :param data: The result data to store
        :param encoding: The encoding to use when storing the string.
                         If data is a string and encoding is omitted, utf-8 will be used as default.
                         If data is a bytes object, the encoding can be dismissed,
                         which enforces the string data to also be read as bytes.
        :param meta_data: A dictionary of optional metadata to store at channel/dataset level.
                          See also 'channel_add_metadata'.
        :param overwrite: If the channel already exists, set this argument to overwrite the data
        """
        self._set_index_out_of_date()
        meta_data = copy.deepcopy(meta_data) or {}
        cname = to_safe_key(cname)

        if cname in grp:
            if overwrite:
                self.log.debug(f"Overwriting dataset {cname}...")
                del grp[cname]
            else:
                msg = f"Dataset {cname} already exists!"
                raise DatasetExistsError(msg)

        if not (encoding is None or exists_encoding(encoding)):
            msg = f"Invalid encoding {encoding!r}!"
            raise LookupError(msg)

        if isinstance(data, str):
            encoding = encoding or "utf-8"
            data = bytes(data, encoding)
            meta_data["encoding"] = encoding
        elif isinstance(data, (bytes, bytearray)):
            if encoding:  # Only store encoding is explicitly given to read stored bytes as string
                meta_data["encoding"] = encoding
        else:
            msg = f"Unsupported data type {type(data)}!"
            raise TypeError(msg)
        ds = grp.create_dataset(cname, data=np.string_(data))

        meta_data["dtype"] = _C.DTypes.String
        meta_data["name"] = str(cname)
        for k, v in meta_data.items():
            node_set_attribute(ds, k, v)
        return ds

    def group_add_array_channel(
        self,
        gid: int | Group,
        cname: str,
        data: np.ndarray | list,
        unit: str | None = "",
        *,
        meta_data: dict[str, Any] | None = None,
        overwrite: bool = True,
        **kwargs,
    ) -> Dataset:
        """
        Adds a new numeric array channel.

        :param gid: The group/iteration number or directly the respective group object.
        :param cname: The channel/dataset name
        :param data: The array data to store
        :param unit: The unit of the result
        :param meta_data: A dictionary of optional metadata to store at channel/dataset level.
                          See also 'channel_add_metadata'.
        :param overwrite: If the channel already exists, set this argument to overwrite the data
        """
        self._set_index_out_of_date()
        meta_data = copy.deepcopy(meta_data) or {}
        grp = self._group_require_node_results(gid)
        cname = to_safe_key(cname)

        if cname in grp:
            if overwrite:
                self.log.debug(f"Overwriting dataset {cname}...")
                del grp[cname]
            else:
                msg = f"Dataset {cname} already exists!"
                raise DatasetExistsError(msg)

        if isinstance(data, list):
            data = np.array(data)

        complex = False
        if not isinstance(data, np.ndarray):
            msg = "Argument data only accepts numpy.ndarray instances!"
            raise TypeError(msg)
        if np.issubdtype(data.dtype, np.complex_):
            complex = True

        datagrp = grp.require_group(cname)
        if complex:
            stacked = np.stack((data.real, data.imag), axis=0)
            ds = datagrp.create_dataset("data", shape=stacked.shape, data=stacked, **self._comp_opts)
            meta_data["dtype"] = _C.DTypes.ComplexNumericArray
        else:
            ds = datagrp.create_dataset("data", shape=data.shape, data=data, **self._comp_opts)
            meta_data["dtype"] = _C.DTypes.NumericArray

        meta_data["name"] = str(cname)
        meta_data["unit"] = str(unit)
        for k, v in meta_data.items():
            node_set_attribute(datagrp, k, v)
        return ds

    def group_add_waveform_channel(
        self,
        gid: int | Group,
        cname: str,
        y_data: np.ndarray | list,
        x_data: np.ndarray | list | tuple[float, float] | None = None,
        unit_y: str | None = "",
        unit_x: str | None = "",
        *,
        meta_data: dict[str, Any] | None = None,
        overwrite: bool = True,
        compress_x: bool = True,
        **kwargs,
    ) -> Dataset:
        """
        Adds a new waveform channel.

        :param gid: The group/iteration number or directly the respective group object.
        :param cname: The channel/dataset name
        :param y_data: The y data to store
        :param x_data: The x data to store
        :param unit_y: The unit of the y axis
        :param unit_x: The unit of the x axis
        :param meta_data: A dictionary of optional metadata to store at channel/dataset level.
                          See also 'channel_add_metadata'.
        :param overwrite: If the channel already exists, set this argument to overwrite the data
        :param compress_x: If True and the x data of the channel is equidistantly sampled, the data will not be stored
                           as dataset, but only metadata for restoring the x data on reading.
        """
        self._set_index_out_of_date()
        meta_data = copy.deepcopy(meta_data) or {}
        grp = self._group_require_node_results(gid)
        cname = to_safe_key(cname)

        if cname in grp:
            if overwrite:
                self.log.debug(f"Overwriting dataset {cname}...")
                del grp[cname]
            else:
                msg = f"Dataset {cname} already exists!"
                raise DatasetExistsError(msg)

        if isinstance(x_data, list):
            x_data = np.array(x_data)
        if isinstance(y_data, list):
            y_data = np.array(y_data)

        if isinstance(x_data, np.ndarray):
            if x_data.ndim != 1:
                msg = "x_data must be a numpy array with a single dimension!"
                raise UnsupportedDataTypeError(msg)
            if len(x_data) == 0:
                msg = "x_data must have at least 1 sample!"
                raise UnsupportedDataTypeError(msg)
        elif isinstance(x_data, tuple):
            if len(x_data) != 2:
                msg = "If x_data is a tuple, it must be in form (x_initial, x_step)"
                raise UnsupportedDataTypeError(msg)
        elif x_data is not None:
            msg = "x_data must be None or of type numpy.ndarray!"
            raise UnsupportedDataTypeError(msg)

        if not isinstance(y_data, np.ndarray):
            msg = "x_data must be None or of type numpy.ndarray!"
            raise UnsupportedDataTypeError(msg)
        if y_data.ndim != 1:
            msg = "y_data must be a numpy array with a single dimension!"
            raise UnsupportedDataTypeError(msg)
        if len(y_data) == 0:
            msg = "y_data must have at least 1 sample!"
            raise UnsupportedDataTypeError(msg)

        if isinstance(x_data, np.ndarray) and isinstance(y_data, np.ndarray) and len(x_data) != len(y_data):
            msg = "y_data and x_data must have the same length!"
            raise UnsupportedDataTypeError(msg)

        x_initial = 0
        if isinstance(x_data, np.ndarray):
            x_initial = x_data[0]
            # Calc samplerate
            length = len(x_data)
            if length == 1:
                samplerate = 0.0
            else:
                distances = np.diff(x_data.astype(np.float64), 1)
                distances -= distances[0]
                if np.issubdtype(distances.dtype, np.integer) or np.all(distances <= 2 * np.spacing(x_data).max()):
                    try:
                        samplerate = float(length - 1) / (x_data[-1] - x_data[0])
                    except Exception:
                        samplerate = 0.0
                else:
                    samplerate = 0.0
        elif isinstance(x_data, tuple):
            x_initial = x_data[0]
            x_step = x_data[1]
            assert x_step > 0, f"({x_initial}, {x_step}): step must be greater than 0!"
            samplerate = 1.0 / x_step
        else:
            samplerate = 1.0

        uniform = samplerate != 0

        datagrp = grp.require_group(cname)
        chunk_size = min(_C.Storage.MaxChunkSize, len(y_data))
        ds = datagrp.create_dataset("data", data=y_data, chunks=chunk_size, **self._comp_opts)
        if uniform:
            # auto chunk size would yield low chunk sizes which results in slightly lower compression rates
            meta_data["sampling_type"] = "uniform"
            meta_data["x_initial"] = x_initial
            meta_data["x_step"] = 1.0 / samplerate
            meta_data["x_rate"] = abs(samplerate)
        else:
            meta_data["sampling_type"] = "non_uniform"
            compress_x = False  # force store x vector

        if not compress_x:
            ds = datagrp.create_dataset(
                "x_data", data=x_data, chunks=min(_C.Storage.MaxChunkSize, len(x_data)), **self._comp_opts
            )

        meta_data["length"] = len(y_data)
        meta_data["dtype"] = _C.DTypes.Waveform
        meta_data["name"] = str(cname)
        meta_data["unit"] = str(unit_y)
        meta_data["unit_y"] = str(unit_y)
        meta_data["unit_x"] = str(unit_x)
        for k, v in meta_data.items():
            node_set_attribute(datagrp, k, v)
        return ds

    def group_add_file_channel(
        self,
        gid: int | Group,
        cname: str,
        file: str | (Path | tuple[FileName, io.BytesIO | bytes]),
        *,
        meta_data: dict[str, Any] | None = None,
        overwrite: bool = True,
        **kwargs,
    ) -> Dataset:
        """
        Adds a new file channel.

        :param gid: The group/iteration number or directly the respective group object.
        :param cname: The channel/dataset name
        :param file: A path to a file or a tuple of filename and bytes data.
        :param meta_data: A dictionary of optional metadata to store at channel/dataset level.
                          See also 'channel_add_metadata'.
        :param overwrite: If the channel already exists, set this argument to overwrite the data
        """
        self._set_index_out_of_date()
        meta_data = copy.deepcopy(meta_data) or {}
        grp = self._group_require_node_results(gid)
        cname = to_safe_key(cname)

        if cname in grp:
            if overwrite:
                self.log.debug(f"Overwriting dataset {cname}...")
                del grp[cname]
            else:
                msg = f"Dataset {cname} already exists!"
                raise DatasetExistsError(msg)

        if isinstance(file, tuple):
            if not (isinstance(file[0], str) and len(file[0]) and isinstance(file[1], (io.BytesIO, bytes))):
                msg = (
                    "If a tuple is passed as argument 'path', the first item has to be a string of at "
                    "least one character and the second item has to be a bytes or io.BytesIO instance!"
                )
                raise TypeError(msg)
            filename, content = file
            if isinstance(content, io.BytesIO):
                content.seek(0)
                data = content.read()
            else:
                data = content
        elif isinstance(file, (str, Path)):
            file = Path(file)
            filename = file.name
            data = file.read_bytes()
        else:
            msg = f"Unsupported type {type(file)} for argument 'file'!"
            raise TypeError(msg)
        fileext = Path(filename).suffix

        datagrp = grp.require_group(cname)
        ds = datagrp.create_dataset(name="data", data=np.void(data))
        meta_data["filename"] = filename
        meta_data["fileext"] = fileext
        meta_data["length"] = len(data)
        meta_data["dtype"] = _C.DTypes.File
        meta_data["name"] = str(cname)
        for k, v in meta_data.items():
            node_set_attribute(datagrp, k, v)
        return ds

    def group_get_channel(
        self, gid: int | Group, cname: str
    ) -> tuple[str | (Number | (Waveform | (np.ndarray | bytes))), dict[str, Any]]:
        """
        Returns a channel and its metadata as dictionary

        :param gid: The group/iteration number or directly the respective group object.
        :param cname: The channel/dataset name
        """
        grp = self._group_get_node_results(gid)
        return self.__get_channel(grp, cname)

    def group_get_channels_data(
        self, gid: int | Group
    ) -> dict[str, tuple[str | (Number | (Waveform | (np.ndarray | bytes)))]]:
        """
        Returns all channels (only data, no metadata) of the group as a dictionary.

        :param gid: The group/iteration number or directly the respective group object.
        """
        return {k: self.group_get_channel(gid, cname=k)[0] for k in self.list_channels(gid)}

    def __get_channel(
        self, grp: Group, cname: str
    ) -> tuple[str | (Number | (Waveform | (np.ndarray | bytes))), dict[str, Any]]:
        """
        Returns a channel and its metadata as dictionary

        :param grp: The respective group object.
        :param cname: The channel/dataset name
        """
        if cname in grp:
            grp_or_ch = grp.get(cname)
            dtype = node_get_attribute(grp_or_ch, "dtype")
            if isinstance(grp_or_ch, Dataset):  # noqa: SIM108
                # Legacy <= 0.5.0, where datasets where located directly in iteration group without
                # Versions > 1.15 use a mix of groups and datasets, since the group overhead should be avoided if only
                # a single dataset is in it
                ds = grp_or_ch
            else:
                ds = grp_or_ch.get("data")  # versions >0.5,<1.15

            if dtype == _C.DTypes.String:
                data = ds[()]
                try:
                    encoding = node_get_attribute(grp_or_ch, "encoding")
                    data = data.decode(encoding)
                except KeyError:
                    data = bytes(data)
            elif dtype == _C.DTypes.Number:
                data = ds[()][0]
            elif dtype == _C.DTypes.ComplexNumber:
                data = ds[()]
                data = np.complex128(complex(data[0][0], data[1][0]))
            elif dtype == _C.DTypes.NumericArray:
                data = ds[()]
            elif dtype == _C.DTypes.ComplexNumericArray:
                data = ds[()]
                data = data[0] + 1j * data[1]
            elif dtype == _C.DTypes.Waveform:
                sampling_type = node_get_attribute(grp_or_ch, "sampling_type")
                if sampling_type == "uniform":
                    y_data = ds[()]
                    x_data = grp_or_ch.get("x_data")
                    if x_data is None:
                        incr = node_get_attribute(grp_or_ch, "x_step")
                        initial = node_get_attribute(grp_or_ch, "x_initial")
                        x_data = np.arange(0, incr * len(y_data), incr)[: len(y_data)] + initial
                    else:
                        x_data = x_data[()]
                elif sampling_type == "non_uniform":
                    _data = ds[()]
                    _shape = _data.shape
                    if len(_shape) == 2:  # Old-style: data and time are in same dataset
                        x_data = _data[0]
                        y_data = _data[1]
                    elif len(_shape) == 1:
                        x_data = grp_or_ch.get("x_data")[()]
                        y_data = _data
                    else:
                        msg = f"Unsupported shape {_data.shape} of dataset!"
                        raise ValueError(msg)
                else:
                    msg = f"Unsupported sampling_type {sampling_type!r}!"
                    raise ValueError(msg)
                data = Waveform(y_data, x_data)
            elif dtype == _C.DTypes.File:
                data = bytes(ds[()])
            else:
                msg = f"Handling of data type {dtype!r} not implemented!"
                raise NotImplementedError(msg)

            return data, node_get_attribute(grp_or_ch, key=None)

        msg = f"Channel {cname!r} does not exist yet. First create a channel using 'channel_add_data'!"
        raise ChannelNotFoundError(msg)

    def static_add_text_data(
        self,
        name: str,
        text: str,
        metaData: dict[str, Any] | None = None,
        overwrite: bool = True,
    ):
        """
        Adds a text dataset to the static section of the file

        :param name: Name of the text dataset
        :param text: The text data
        :param metaData: A dictionary of metadata to store with the text dataset.
        :param overwrite: If the key already exists, set this argument to overwrite the data
        """
        name = to_safe_key(name)
        if name in self.static_grp:
            if overwrite:
                self.log.debug(f"Overwriting dataset {name}...")
                del self.static_grp[name]
            else:
                msg = f"Dataset {name} already exists!"
                raise DatasetExistsError(msg)

        ds = self.static_grp.create_dataset(name=name, data=np.string_(bytes(str(text), "utf-8")))
        metaData = copy.deepcopy(metaData) or {}
        for k, v in metaData.items():
            node_set_attribute(ds, k, v, overwrite)

    def static_get_text_data(self, name: str) -> tuple[str, dict[str, Any]]:
        """
        Returns a text dataset from the static section of the file

        :param name: Name of the text dataset
        """
        if name in self.static_grp:
            ds = self.static_grp[name]
            metaData = dict(node_get_attribute(ds, key=None))
            text = ds[()].decode("ascii")
            return text, metaData

        msg = f"Cannot find static dataset {name!r}!"
        raise DatasetExistsError(msg)

    def list_channels(self, gid: int | Group) -> list[str]:
        """
        Lists all channels of a specific group

        :param gid: The group/iteration number or directly the respective group object.
        """
        grp = self._group_get_node_results(gid)
        return list(grp.keys())

    def list_waveform_channels(self, gid: int | Group) -> list[str]:
        """
        Lists all channels of the desired group that are of type "waveform"

        :param gid: The group/iteration number or directly the respective group object.
        """
        grp = self._group_get_node_results(gid)

        names = []
        for ds in grp.values():
            if node_get_attribute(ds, key="dtype") == _C.DTypes.Waveform:
                names.append(node_get_attribute(ds, key="name"))
        return names

    def list_numeric_channels(self, gid: int | Group) -> list[str]:
        """
        Lists all channels of the desired group that are of type "number"

        :param gid: The group/iteration number or directly the respective group object.
        """
        grp = self._group_get_node_results(gid)

        names = []
        for ds in grp.values():
            if node_get_attribute(ds, key="dtype") in (
                _C.DTypes.Number,
                _C.DTypes.ComplexNumber,
            ):
                names.append(node_get_attribute(ds, key="name"))
        return names

    def list_array_channels(self, gid: int | Group) -> list[str]:
        """
        Lists all channels of the desired group that are of type "numeric_array"

        :param gid: The group/iteration number or directly the respective group object.
        """
        grp = self._group_get_node_results(gid)

        names = []
        for ds in grp.values():
            if node_get_attribute(ds, key="dtype") in (
                _C.DTypes.NumericArray,
                _C.DTypes.ComplexNumericArray,
            ):
                names.append(node_get_attribute(ds, key="name"))
        return names

    def list_string_channels(self, gid: int | Group) -> list[str]:
        """
        Lists all channels of the desired group that are of type "string"

        :param gid: The group/iteration number or directly the respective group object.
        """
        grp = self._group_get_node_results(gid)

        names = []
        for ds in grp.values():
            if node_get_attribute(ds, key="dtype") == _C.DTypes.String:
                names.append(node_get_attribute(ds, key="name"))
        return names

    def list_file_channels(self, gid: int | Group) -> list[str]:
        """
        Lists all channels of the desired group that are of type "file"

        :param gid: The group/iteration number or directly the respective group object.
        """
        grp = self._group_get_node_results(gid)

        names = []
        for ds in grp.values():
            if node_get_attribute(ds, key="dtype") == _C.DTypes.File:
                names.append(node_get_attribute(ds, key="name"))
        return names

    def get_index_operating_conditions(self) -> Index:
        """
        Returns the operating conditions index that can be further filtered
        """
        self._check_index_out_of_date()
        try:
            index = json.loads(self.static_get_text_data(_C.Indexes.OperatingConditions)[0])
        except DatasetExistsError:  # pragma: no cover
            index = []
        return Index(index)

    def get_index_metadata(self) -> Index:
        """
        Returns the metadata index that can be further filtered
        """
        self._check_index_out_of_date()
        try:
            index = json.loads(self.static_get_text_data(_C.Indexes.Metadata)[0])
        except DatasetExistsError:  # pragma: no cover
            index = []
        return Index(index)

    def get_index_results(self) -> Index:
        """
        Returns the results index that can be further filtered
        """
        self._check_index_out_of_date()
        try:
            index = json.loads(self.static_get_text_data(_C.Indexes.Results)[0])
        except DatasetExistsError:  # pragma: no cover
            index = []
        return Index(index)

    def get_index_results_metadata(self) -> Index:
        """
        Returns the result metadata index that can be further filtered
        """
        self._check_index_out_of_date()
        try:
            index = json.loads(self.static_get_text_data(_C.Indexes.ResultsMetadata)[0])
        except DatasetExistsError:  # pragma: no cover
            index = []
        return Index(index)

    @staticmethod
    def export_subset(
        source: DLH5 | (Path | io.IOBase), dest: Path | io.IOBase, groups: list[int], channels: list[str]
    ):
        """
        Exports a subset of the file as new DLH5 file.

        :param source: The source file. Either a DLH5 instance of a path to a DLH5 file.
        :param groups: A list of group ids to export
        :param channels: A list of channel names to export
        """
        if not len(groups):  # pragma: no cover
            msg = "Not enough groups specified!"
            raise ValueError(msg)
        if not len(channels):  # pragma: no cover
            msg = "Not enough channels specified!"
            raise ValueError(msg)

        src = _open_dlh5(source, mode=DLH5.Mode.ReadOnly)
        opened = src.open()  # Open if not already open
        try:
            with DLH5(dest, src.Mode.Write, auto_create_index=False) as dst:
                # Copy file metadata
                for k, v in src.file_get_metadata().items():
                    with contextlib.suppress(MetadataExistsError):
                        dst.file_add_metadata(k, v, overwrite=False)

                for gid in groups:
                    try:
                        src_grp = src.file_get_group(gid)
                    except LookupError:
                        continue

                    # Shallow copy group with metadata
                    src.file.copy(src_grp, dst.file[src_grp.parent.name], shallow=True)

                    # Copy operating conditions
                    src_grp_op = src_grp[_C.H5Groups.OpCond]
                    for op in src_grp_op:
                        src_grp_op.copy(op, dst.file[src_grp_op.name])

                    # Copy channels
                    src_results_grp = src._group_get_node_results(gid)
                    dst_results_grp = dst.file[src_grp.name].require_group("results")
                    for ch in channels:
                        if ch in src_results_grp:
                            src_results_grp.copy(ch, dst_results_grp)

                dst.create_indexes()

        finally:
            if opened:
                src.close(skip_index=True)

    @staticmethod
    def jsonify_channel_data(data: str | (Number | (Waveform | np.ndarray)), meta: dict[str, Any]) -> dict[str, Any]:
        meta = meta or {}
        data_jsonified = {
            "meta": meta,
        }
        if isinstance(data, Waveform):
            data_jsonified["x_data"] = data.time.tolist()
            data_jsonified["y_data"] = data.data.tolist()
            data_jsonified["x_np_dtype"] = str(data.time.dtype)
            data_jsonified["y_np_dtype"] = str(data.data.dtype)
        else:
            msg = f"Data type {type(data)} not supported yet!"
            raise NotImplementedError(msg)
        return data_jsonified

    @staticmethod
    def merge_into(
        merge_target: str | (Path | io.IOBase),
        merge_source: str | (Path | io.IOBase),
        extlink: Literal["no", "abs", "rel"] = "no",
        overwrite: bool = False,
        auto_create_index: bool = True,
        merge_hook: Callable[[DLH5, DLH5], None] | None = None,
        strategy: Literal["append", "merge_by_opcond"] = "append",
    ):
        """
        Merges the DLH5 file ``merge_source`` into ``merge_target``.

        The file metadata from the target act as base, but the source may add new metadata in the merge process if it
        does not exist in the target.

        Merging is done by appending the groups of the source files after the last group of the target file,
        e.g. ``[0,1,2] + [0,1,2] = [0,1,2,3,4,5]``.

        :param merge_target: The file which acts as merge target. Must be an existing DLH5 file.
        :param merge_source: The file to be merged into merge_target.
        :param extlink: Control how the groups of the source files are put into the target file.

            ``"no"``
                Copy groups into target file.

            ``"rel"``
                Link groups with relative links. This is typically used if the merged file should act as "access" layer
                for multiple source files.

                For the target file being readable after merging,
                the source files have to be kept relative to the target file.

            ``"abs"``
                Link groups with absolute links. This is typically used if the merged file should act as "access" layer
                for multiple source files.

                For the target file being readable after merging,
                the source files must not be to moved after merging, the target file may.

            If ``"rel"`` or ``"abs"`` is used, files to be merged must be from disk and not be in-memory DLH5 files
            (io.IOBase).
        :param overwrite: If True, metadata and static text data from 'merge_source' will overwrite equally named
            keys in 'merge_target'.
        :param auto_create_index: Set to True if the index shall be re-generated after merging.
            If this function is directly called, this should be set to True.
            :func:`dlh5.api.DLH5.merge` is using this method internally, where it sets the argument to False
            for speed optimization.
        :param merge_hook: A callable that gets the target and source DLH5 file instance. It can be used to implement
            additional user-defined merge actions on file level (e.g. merging static test data).

            Example::

                def merge_log(tgt: DLH5, src: DLH5):
                    tgt.static_add_text_data(
                        "log",
                        tgt.static_get_text_data("log")[0] + src.static_get_text_data("log")[0]
                    )

                DLH5.merge_into(f1, f2, merge_hook=merge_log)
        :param strategy: The merge strategy to use. Can be either ``"append"`` or ``"merge_by_opcond"``.

            ``"append"`` means that the groups of the source files are appended after the last group of the target file.

            ``"merge_by_opcond"`` means that the groups of the target files are replaced by groups of the source files
            by matching the operating conditions. If no group with the same operating conditions is found, a new group
            is created.
        """
        if extlink not in ["no", "abs", "rel"]:
            msg = "Invalid value for 'extlink'!"
            raise ValueError(msg)
        extlink_enabled = extlink != "no"
        if extlink_enabled and not isinstance(merge_source, (str, Path)):
            msg = "External links are only allowed if 'merge_source' is of type str|Path!"
            raise TypeError(msg)

        with (
            DLH5(merge_target, mode=DLH5.Mode.Append, auto_create_index=auto_create_index) as target,
            DLH5(merge_source, mode=DLH5.Mode.ReadOnly) as source,
        ):
            for k, v in source.file_get_metadata().items():
                # Copy file level metadata if they don't exist on merge target
                with contextlib.suppress(MetadataExistsError):
                    target.file_add_metadata(k, v, overwrite=overwrite)

            # Copy static data
            tgt_statics = set(target.static_grp.keys())
            for k in source.static_grp:
                if k not in tgt_statics or overwrite:
                    text, meta = source.static_get_text_data(k)
                    target.static_add_text_data(k, text, meta, overwrite=overwrite)

            if callable(merge_hook):
                merge_hook(target, source)

            if strategy == "append":
                # Append new groups
                tgt_groups = list(target.data_grp.keys())
                last_group_index = max(map(int, tgt_groups)) if len(tgt_groups) > 0 else -1

                for i, src_gid in enumerate(source.file_list_groups()):
                    src_grp = source.file_get_group(src_gid)
                    tgt_gid = _gid2str(last_group_index + 1 + i)
                    if extlink_enabled:
                        if isinstance(merge_target, (str, Path)) and extlink == "rel":
                            rel_path = os.path.relpath(merge_source, merge_target.parent)
                            target.data_grp[tgt_gid] = ExternalLink(rel_path, src_grp.name)
                        else:
                            target.data_grp[tgt_gid] = ExternalLink(merge_source, src_grp.name)
                    else:
                        source.file.copy(src_grp, target.data_grp, name=tgt_gid)
                    target._set_index_out_of_date()

            elif strategy == "merge_by_opcond":
                # Replace groups by operating conditions
                for i, src_gid in enumerate(source.file_list_groups()):  # noqa: B007
                    src_opcond = {k: v[0] for k, v in source.group_get_operating_conditions(src_gid).items()}
                    target_gids = target.file_find_groups_by_opcond(src_opcond)
                    if len(target_gids) > 1:
                        msg = f"Found none or multiple groups ({target_gids}) for operating conditions {src_opcond}!"
                        raise Exception(msg)
                    if len(target_gids) == 1:
                        tgt_gid = _gid2str(target_gids[0])
                    else:
                        tgt_groups = list(target.data_grp.keys())
                        last_group_index = max(map(int, tgt_groups)) if len(tgt_groups) > 0 else -1
                        tgt_gid = _gid2str(last_group_index + 1)
                    src_grp = source.file_get_group(src_gid)

                    if tgt_gid in target.data_grp:
                        del target.data_grp[tgt_gid]
                    if extlink_enabled:
                        if isinstance(merge_target, (str, Path)) and extlink == "rel":
                            rel_path = os.path.relpath(merge_source, merge_target.parent)
                            target.data_grp[tgt_gid] = ExternalLink(rel_path, src_grp.name)
                        else:
                            target.data_grp[tgt_gid] = ExternalLink(merge_source, src_grp.name)
                    else:
                        source.file.copy(src_grp, target.data_grp, name=tgt_gid)
                if i:
                    target._set_index_out_of_date()

    def verify_external_links(self):
        """
        Verifies if all external links to iteration groups are still valid.

        :raises: BrokenExternalLinkError
        """
        missing = []
        for gid in self.file_list_groups():
            try:
                self.file_get_group(gid)
            except KeyError:
                missing.append(gid)
        if len(missing):
            raise BrokenExternalLinkError("External links to groups " + ",".join(map(str, missing)) + " are broken!")

    @staticmethod
    def merge(
        target: str | Path,
        files: Iterable[str | (Path | io.IOBase)],
        extlink: Literal["no", "abs", "rel"] = "no",
        merge_hook: Callable[[DLH5, DLH5], None] | None = None,
        strategy: Literal["append", "merge_by_opcond"] = "append",
    ) -> Path:
        """
        Takes an iterable of DLH5 files and merges them one by one into a target DLH5 file on disk.

        The first file's metadata will act as the basis, but other files may add new metadata in the merge process if it
        does not exist in the target.

        Merging is done by appending the groups of the source files after the last group of the target file,
        e.g. ``[0,1,2] + [0,1,2] = [0,1,2,3,4,5]``.

        :param target: A not existing file where all DLH5 files are merged into.
        :param files: An iterable of DLH5 files to merge. Must contain at least one item.
        :param extlink: Control how the groups of the source files are put into the target file.

            ``"no"``
                Copy groups into target file.

            ``"rel"``
                Link groups with relative links. This is typically used if the merged file should act as "access layer"
                for multiple source files.

                For the target file being readable after merging,
                the source files have to be kept relative to the target file.

            ``"abs"``
                Link groups with absolute links. This is typically used if the merged file should act as "access layer"
                for multiple source files.

                For the target file being readable after merging,
                the source files must not be to moved after merging, the target file may.

            If ``"rel"`` or ``"abs"`` is used, files to be merged must be from disk and not be in-memory DLH5 files
            (io.IOBase).
        :param merge_hook: A callable that gets the target and source DLH5 file instance. It can be used to implement
            additional user-defined merge actions on file level (e.g. merging static test data).

            Example::

                def merge_log(tgt: DLH5, src: DLH5):
                    tgt.static_add_text_data(
                        "log",
                        tgt.static_get_text_data("log")[0] + src.static_get_text_data("log")[0]
                    )

                DLH5.merge(outfile, [f1, f2], merge_hook=merge_log)
        :param strategy: The merge strategy to use. Can be either ``"append"`` or ``"merge_by_opcond"``.

            ``"append"`` means that the groups of all source files are appended after one another.

            ``"merge_by_opcond"`` means that the groups of the target files are replaced by groups of the source files
            by matching the operating conditions. If no group with the same operating conditions is found, a new group
            is created.
        """
        target = Path(target)
        if target.exists():
            msg = f"Target file {target} must not exist!"
            raise FileExistsError(msg)

        i = 0
        files_iter = iter(files)
        while True:
            try:
                source_file = next(files_iter)
            except StopIteration:
                break

            if i == 0:
                # Create an empty file where stuff will be merged into
                target.parent.mkdir(parents=True, exist_ok=True)
                with DLH5(target, "w", auto_create_index=False):
                    pass

            DLH5.merge_into(
                merge_target=target,
                merge_source=source_file,
                extlink=extlink,
                overwrite=i == 0,  # On first merge, target is a dummy file, so overwrite all metadata once
                auto_create_index=False,
                merge_hook=merge_hook if i > 0 else None,
                strategy="append" if i == 0 else strategy,  # First file always gets appended to new empty file
            )
            i += 1

        with DLH5(target, DLH5.Mode.Append, auto_create_index=False) as f:
            f.create_indexes()

        return target

    @staticmethod
    def convert_from_xvpmat(src: str | Path, dst: str | Path | None = None, **kwargs) -> Path:
        """
        Converts an existing XVP MAT file to DLH5 format.

        Additional keyword arguments are passed on to :meth:`dlh5.api.DLH5.__init__`.

        :param src: The source file to convert
        :param dst: The target path. If omitted, the src file name will be used with a ``.dlh5`` suffix.
        """
        from .converters import xvpmat_to_dlh5

        return xvpmat_to_dlh5(src, dst, **kwargs)

    @staticmethod
    def convert_from_pscvmat(src: str | Path, dst: str | Path | None = None, **kwargs) -> Path:
        """
        Converts an existing PS CV MAT file to DLH5 format.

        Additional keyword arguments are passed on to :meth:`dlh5.api.DLH5.__init__`.

        :param src: The source file to convert
        :param dst: The target path. If omitted, the src file name will be used with a ``.dlh5`` suffix.
        """
        from .converters import pscvmat_to_dlh5

        return pscvmat_to_dlh5(src, dst, **kwargs)

    @staticmethod
    def convert_from_atvsctdms(src: str | Path, dst_dir: str | Path | None = None, **kwargs) -> list[Path]:
        """
        Converts an ATV SC TDMS result file to multiple DLH5 files (one for each test).

        So for a TDMS file with multiple tests like this::

            Brown Out-0f94a977-985b-467b-a5b1-9b35618cd27f
            Measure Jitter-38a66d63-9515-464f-b335-2da023873f67
            Brown Out-90891dd2-6a58-464c-be13-96e8132bca77
            Measure Jitter-04cd1b21-4446-4e5c-8274-1bae70f6e41d

        two DLH5 files will be created in the destination directory, one for each unique test name::

            Brown Out.dlh5
            Measure Jitter.dlh5

        """
        from .converters import atvsc_tdms_to_dlh5

        return atvsc_tdms_to_dlh5(src, dst_dir, **kwargs)

    @staticmethod
    def save_file_channel(data: bytes, metadata: dict, dest: str | Path, prefix: str = "") -> Path:
        """
        Takes the content (bytes data and metadata) returned by :func:`dlh5.api.group_get_channel` and
        saves it to a file. Example::

            data, meta = dlh5file.group_get_channel(0, "myfile")
            dest = dlh5file.save_file_channel(data, meta, "export/custom_file_name.txt")

        :param data: Bytes data returned by :func:`dlh5.api.group_get_channel`
        :param metadata: Metadata returned by :func:`dlh5.api.group_get_channel`
        :param dest: Either a file path or a directory to save the file. If it is a directory the file name will be
                     taken from the metadata. If the path or its parent folders do not exist, they will be created.
        :param prefix: The prefix that shall be added to the written file
        """
        if metadata.get("dtype", "") != _C.DTypes.File:
            msg = "Can only save data from file channels"
            raise TypeError(msg)
        dest = Path(dest)
        if dest.suffix:
            dest = dest.with_name(prefix + dest.name)
            dest.parent.mkdir(parents=True, exist_ok=True)
            dest.write_bytes(data)
        else:
            assert "filename" in metadata, "metadata must contain the key 'filename' if 'dest' is a directory!"
            dest.mkdir(parents=True, exist_ok=True)
            dest = Path(dest / metadata["filename"])
            dest = dest.with_name(prefix + dest.name)
            dest.write_bytes(data)
        return dest

    @staticmethod
    def dump_file_channels(source: DLH5 | (Path | (str | io.IOBase)), dest: Path | str, nested: bool = False):
        src = _open_dlh5(source, mode=DLH5.Mode.ReadOnly)
        opened = src.open()  # Open if not already open
        try:
            dest = Path(dest)

            for gid in src.file_list_groups():
                grp = src.file_get_group(gid)

                prefix = _gid2str(gid)
                new_dest = (dest / prefix) if nested else dest
                new_dest.mkdir(parents=True, exist_ok=True)

                context = {"op": src.group_get_operating_conditions(grp), "meta": src.group_get_metadata(grp)}
                with open(new_dest / f"{prefix}_context.json", "w") as f:
                    json.dump(context, f, default=json_default_serializer, indent=2)

                for cname in src.list_file_channels(grp):
                    data, meta = src.group_get_channel(grp, cname)
                    DLH5.save_file_channel(data, meta, new_dest, prefix=prefix + "_")
        finally:
            if opened:
                src.close()

    compress = staticmethod(compress)

    uncompress = staticmethod(uncompress)


def _open_dlh5(source: DLH5 | (Path | (str | io.IOBase)), **kwargs) -> DLH5:
    if isinstance(source, (Path, str, io.IOBase)):
        return DLH5(source, **kwargs)
    if isinstance(source, DLH5):
        return source

    msg = "Argument src must be of type DLH5, pathlib.Path, str or io.IOBase!"
    raise TypeError(msg)


def _gid2str(gid: int) -> str:
    return f"{gid:>08}"
