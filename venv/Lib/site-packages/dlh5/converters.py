from __future__ import annotations

import io
import re
from pathlib import Path

import numpy as np

from dlh5 import DLH5
from dlh5.telemetry import track_feature


def mat_to_dlh5(src: str | Path, dst: str | (Path | io.IOBase) | None = None, **kwargs) -> Path:
    try:
        from scipy.io import loadmat
    except ImportError:
        msg = (
            "mat_to_dlh5 needs the scipy.io package installed to read MAT files. Please install via "
            "PIP or install the dlh5 library with the xvp extra: pip install dlh5[mat]"
        )
        raise ImportError(msg) from None

    src = Path(src)
    if not src.exists():
        msg = f"Cannot find file {src}!"
        raise FileNotFoundError(msg)

    if isinstance(dst, (str, Path)):
        dst = Path(dst)
    elif isinstance(dst, io.IOBase):
        pass
    else:
        dst = src.with_suffix(".dlh5")

    mat = loadmat(str(src), squeeze_me=True, chars_as_strings=True, simplify_cells=True)

    if "data" in mat:
        kwargs.setdefault("zip", False)
        kwargs.setdefault("zip_inplace", True)

        try:
            mat["data"]["meta"]["sw"]["ps_cvsv_common_instruments"]
        except Exception:
            # Not a PS CV MAT file
            ...
        else:
            # PS CV MAT file
            return pscvmat_to_dlh5(src, dst, **kwargs)

        return _load_xvpmat(mat, src, dst, **kwargs)

    return _load_xvpmat(mat, src, dst, **kwargs)


def _load_xvpmat(mat: dict, src: Path, dst: str | (Path | io.IOBase), **kwargs) -> Path:
    for k, v in mat.items():
        if not k.startswith("__"):
            if isinstance(v, dict):
                v = [v]  # XVP pspice flavor  # noqa: PLW2901
            root: list = v
            break
    else:
        msg = f"Cannot find top-level root item in keys {','.join(mat.keys())}"
        raise LookupError(msg)

    kwargs.setdefault("zip_inplace", True)

    with DLH5(dst, mode=DLH5.Mode.Write, **kwargs) as f:
        f.file_add_metadata("source", str(src.name))

        for i, iteration in enumerate(root):
            gid = f.file_require_group(i)

            for k, v in iteration.items():
                if isinstance(v, np.ndarray):
                    v = "".join(v)  # noqa: PLW2901

                if k == "XVPINTERNAL_platform":  # noqa: SIM102
                    if v.lower().startswith("igxl"):
                        # Automatically enable zip for IGXL files if not explicitly disabled
                        f._zip_enabled = kwargs.get("zip", True)
                if k.lower() in ("true", "false"):
                    continue
                if k.startswith(("XVP", "metadata_")):
                    if isinstance(v, (int, str, bool)):
                        f.group_add_metadata(gid, k, v)
                elif isinstance(v, (str,)):
                    f.group_add_string_channel(gid, k, v, encoding="utf-8")
                elif isinstance(v, (int, float, bool)):
                    f.group_add_numeric_channel(gid, k, v)
                elif isinstance(v, dict):
                    if "y" not in v:
                        continue
                    for time_key in ("t", "x"):
                        x_data = v.get(time_key)
                        if x_data is not None:
                            break
                    else:
                        x_data = []
                    f.group_add_waveform_channel(gid, k, y_data=v["y"], x_data=x_data)

    track_feature("xvpmat_to_dlh5")
    return dst


def xvpmat_to_dlh5(src: str | Path, dst: str | (Path | io.IOBase) | None = None, **kwargs) -> Path:
    try:
        from scipy.io import loadmat
    except ImportError:
        msg = (
            "xvpmat_to_dlh5 needs the scipy.io package installed to read MAT files. Please install via "
            "PIP or install the dlh5 library with the xvp extra: pip install dlh5[mat]"
        )
        raise ImportError(msg) from None

    src = Path(src)
    if not src.exists():
        msg = f"Cannot find file {src}!"
        raise FileNotFoundError(msg)

    if isinstance(dst, (str, Path)):
        dst = Path(dst)
    elif isinstance(dst, io.IOBase):
        pass
    else:
        dst = src.with_suffix(".dlh5")

    mat = loadmat(str(src), squeeze_me=True, chars_as_strings=True, simplify_cells=True)
    return _load_xvpmat(mat, src, dst, **kwargs)


def _flatten_dict(nested_dict, parent_key="", sep="."):
    """
    Flattens a nested dictionary into a flat dictionary with keys concatenated with a dot.
    """
    items = []
    for k, v in nested_dict.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(_flatten_dict(v, new_key, sep).items())
        else:
            items.append((new_key, v))
    return dict(items)


def _load_pscvmat(mat: dict, src: Path, dst: str | (Path | io.IOBase), **kwargs) -> Path:
    data = mat["data"]

    with DLH5(dst, mode=DLH5.Mode.Write, **kwargs) as f:
        f.file_add_metadata("source", str(src.name))

        gid = f.file_require_group(0)

        for cond_name, cond in data["cond"].items():
            f.group_add_operating_condition(gid, cond_name, cond["value"], cond["unit"])

        meta = _flatten_dict(
            {
                "meas": data["meta"].get("meas", {}),
                "dut": data["meta"].get("dut", {}),
            }
        )
        for k, v in meta.items():
            f.group_add_metadata(gid, k, v)

        for k, v in data["wfm"].items():
            y = v["y"]
            f.group_add_waveform_channel(gid, k, y, (v["initX"], v["incrX"]))

    track_feature("pscvmat_to_dlh5")
    return dst


def pscvmat_to_dlh5(src: str | Path, dst: str | (Path | io.IOBase) | None = None, **kwargs) -> Path:
    try:
        from scipy.io import loadmat
    except ImportError:
        msg = (
            "pscvmat_to_dlh5 needs the scipy.io package installed to read MAT files. Please install via "
            "PIP or install the dlh5 library with the xvp extra: pip install dlh5[mat]"
        )
        raise ImportError(msg) from None

    src = Path(src)
    if not src.exists():
        msg = f"Cannot find file {src}!"
        raise FileNotFoundError(msg)

    if isinstance(dst, (str, Path)):
        dst = Path(dst)
    elif isinstance(dst, io.IOBase):
        pass
    else:
        dst = src.with_suffix(".dlh5")

    kwargs.setdefault("zip", False)

    mat = loadmat(str(src), squeeze_me=True, chars_as_strings=True, simplify_cells=True)
    return _load_pscvmat(mat, src, dst, **kwargs)


def atvsc_tdms_to_dlh5(src: str | Path, dst_dir: str | Path | None = None, **kwargs) -> list[Path]:
    """
    Converts an ATV SC TDMS result file to multiple DLH5 files (one for each test).

    So for a TDMS file with multiple tests like this::

        Brown Out-0f94a977-985b-467b-a5b1-9b35618cd27f
        Measure Jitter-38a66d63-9515-464f-b335-2da023873f67
        Brown Out-90891dd2-6a58-464c-be13-96e8132bca77
        Measure Jitter-04cd1b21-4446-4e5c-8274-1bae70f6e41d

    two DLH5 files will be created in the destination directory, one for each unique test name::

        Brown Out.dlh5
        Measure Jitter.dlh5

    """
    try:
        from nptdms import TdmsFile
    except ImportError:
        msg = (
            "atvsc_tdms_to_dlh5 needs the nptdms package installed to read TDMS files. Please install via "
            "PIP or install the dlh5 library with the TDMS extra: pip install dlh5[tdms]"
        )
        raise ImportError(msg) from None

    src = Path(src)
    if not src.exists():
        msg = f"Cannot find file {src}!"
        raise FileNotFoundError(msg)

    if isinstance(dst_dir, (str, Path)):
        dst_dir = Path(dst_dir)
    elif dst_dir is None:
        dst_dir = src.parent
    else:
        msg = "dst_dir must be a string or pathlib.Path instance, pointing to a directory."
        raise TypeError(msg)
    dst_dir.mkdir(parents=True, exist_ok=True)

    kwargs.setdefault("zip", False)

    # Test names are always unique and end with a unique UUID suffix
    # So the test names can be used to group test runs from different global sweeps like over temperature
    test_name_regex = re.compile(r"^(.+)-([a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12})$")
    dlh5_file_handles: dict[str, DLH5] = {}  # Test name -> DLH5 file handle

    try:
        with TdmsFile.open(src) as tdms_file:
            file_metadata = tdms_file.properties
            for tdms_group in tdms_file.groups():
                match = test_name_regex.match(tdms_group.name)
                if not match:
                    # Ignore non-test groups like VCSInfo and Instruments group for now
                    continue
                test_name, test_uuid = match.groups()
                group_metadata = tdms_group.properties

                # Open a new DLH5 file for each unique test name
                if test_name not in dlh5_file_handles:
                    dlh5_file_handles[test_name] = dlh5_file = DLH5(
                        dst_dir / f"{test_name}.dlh5", mode=DLH5.Mode.Write, **kwargs
                    )

                    for key, value in file_metadata.items():
                        dlh5_file.file_add_metadata(key, value)
                    dlh5_file.file_add_metadata("TdmsSourceFile", str(src.name))
                    dlh5_file.file_add_metadata("MeasurementName", test_name)
                else:
                    dlh5_file = dlh5_file_handles[test_name]

                dlh5_group_id = len(dlh5_file.file_list_groups())

                # First check how many variations were done by checking the channels with source "Variation"
                # and comparing the length of the data to the DataSetLength property
                _variations = []
                for tdms_ch in tdms_group.channels():
                    tdms_ch_metadata = tdms_ch.properties
                    tdms_ch_is_variation = tdms_ch_metadata["Source"].lower() == "variation"
                    if tdms_ch_is_variation:
                        _variations.append(int(len(tdms_ch[:]) / tdms_ch_metadata["DataSetLength"]))
                tdms_num_variations = max(_variations) if _variations else 1

                for tdms_variation_index in range(tdms_num_variations):
                    # Potentially create multiple DLH5 groups for a single TDMS channel,
                    # depending on the data layout (in TDMS N-dim data from M variations is flattened to 1x(N*M) array)
                    for key, value in group_metadata.items():
                        dlh5_file.group_add_metadata(dlh5_group_id, key, value)
                    dlh5_file.group_add_metadata(dlh5_group_id, "MeasurementUUID", test_uuid)
                    # todo: the same metadata may be added for each iteration.
                    #  Maybe move to file-level later in postprocessing.

                    for tdms_ch in tdms_group.channels():
                        tdms_ch_metadata = tdms_ch.properties
                        tdms_ch_data = tdms_ch[:]  # We must read the data to get its length
                        tdms_ch_length = len(tdms_ch_data)
                        # Either DataSetLength or DataColumnLength and DataRowLength is set
                        tdms_ch_datasetlength = tdms_ch_metadata.get("DataSetLength", 0)
                        tdms_ch_datacolumnlength = tdms_ch_metadata.get("DataColumnLength", 0)
                        tdms_ch_datarowlength = tdms_ch_metadata.get("DataRowLength", 0)
                        tdms_ch_unit = tdms_ch_metadata.get("unit_string", tdms_ch_metadata.get("Unit", ""))
                        tdms_ch_is_variation = tdms_ch_metadata["Source"].lower() == "variation"

                        if tdms_ch_is_variation:
                            data = tdms_ch_data[tdms_variation_index] if tdms_ch_length > 1 else tdms_ch_data[0]
                            dlh5_file.group_add_operating_condition(dlh5_group_id, tdms_ch.name, data)
                        elif tdms_ch_datasetlength > 0:
                            data = tdms_ch_data[
                                tdms_variation_index * tdms_ch_datasetlength : (tdms_variation_index + 1)
                                * tdms_ch_datasetlength
                            ]
                            if len(data) == 1:  # Scalar result
                                data = data[0]
                                dlh5_file.group_add_numeric_channel(
                                    dlh5_group_id, tdms_ch.name, data, tdms_ch_unit, meta_data=tdms_ch_metadata
                                )
                            else:  # 1-dim time series
                                # todo: read time vector once standardized by Alex
                                dlh5_file.group_add_waveform_channel(
                                    dlh5_group_id, tdms_ch.name, data, unit_y=tdms_ch_unit, meta_data=tdms_ch_metadata
                                )

                        elif tdms_ch_datacolumnlength > 0 and tdms_ch_datarowlength > 0:
                            # n-dim array
                            data = tdms_ch_data[
                                tdms_variation_index * tdms_ch_datarowlength * tdms_ch_datacolumnlength : (
                                    tdms_variation_index + 1
                                )
                                * tdms_ch_datarowlength
                                * tdms_ch_datacolumnlength
                            ]
                            dlh5_file.group_add_array_channel(
                                dlh5_group_id, tdms_ch.name, data, meta_data=tdms_ch_metadata
                            )
                        else:
                            msg = f"Unsupported channel type: {tdms_ch_metadata}"
                            raise AttributeError(msg)

                    dlh5_group_id += 1

    finally:
        files = []
        for f in dlh5_file_handles.values():
            files.append(f.fileio)
            f.close()
    return files


def any_to_dlh5(src: str | Path, dst: str | Path | None = None, **kwargs) -> Path | list[Path]:
    """
    Converts any supported file format to DLH5.
    """
    source = Path(src)
    if source.suffix.lower() == ".mat":
        return mat_to_dlh5(source, dst, **kwargs)
    if source.suffix.lower() == ".tdms":
        return atvsc_tdms_to_dlh5(source, dst, **kwargs)

    msg = "Unsupported file format!"
    raise ValueError(msg)
