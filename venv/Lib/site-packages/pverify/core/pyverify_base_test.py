# ruff: noqa: E402
from __future__ import annotations

import inspect
import logging
import numbers
import os
import pprint
import sys
import time
import traceback
import unittest.mock
import uuid
import warnings
from collections import Counter
from os.path import abspath, basename, dirname, exists, join
from pathlib import Path
from typing import Any

import arrow
import dlh5
from wavewatson import Waveform

logging.captureWarnings(True)
import re

os.environ["GIT_PYTHON_REFRESH"] = "quiet"  # Mute error if no git installed
import argparse
import gc
import json
import platform
import shutil
import textwrap
from getpass import getuser
from multiprocessing import Pipe
from textwrap import dedent
from threading import Thread
from time import sleep

import git
from numpy import isinf, isnan, ndarray

from pverify import __version__
from pverify.integrations.teehouse.client import TEEHouseClient
from pverify.integrations.xhub import xHub

from . import env
from .hooks import DoE, HookCaller
from .internals import flow_control, logging_util
from .internals.attributes import Attributes
from .internals.Enums import Enums
from .internals.limits import Limits
from .internals.logging_util import get_test_logger
from .internals.outputs import Output, Outputs
from .internals.parameter_value_change_callbacks import ParameterValueChangeCallbacks
from .internals.parameters import Parameter, Parameters
from .internals.postproconly import postproc_mode_handler
from .internals.reporting import Reporting
from .internals.results import ResultContainer, Results, ResultSummary
from .internals.run_options import RunOptions
from .internals.specification import Specification, SpecificationResult
from .internals.specification_checker import SpecificationChecker
from .internals.storage_options import StorageOptions
from .internals.sweep_manager import SweepManager
from .internals.unit import ScalarUnit
from .internals.utils import IterationInterruptedError, Utils
from .io.result_storage.result_gateway import ResultGateway
from .util import deleter, dict_util, fnmatch_filter, json_encoder, numeric_util, path_util, string_util, telemetry


class PyVerifyBaseTest:
    """
    The central base class for all PyVerify tests.
    """

    __test__ = False

    def __init__(self, *args, **kwargs):
        r"""
        Constructor

        :keyword result_dir: Defines the result directory for the test. Multiple formatting options can be used:
            testname, testcasename, time, time\_, date, date\_, user, host and time-related formatters
            (%a, %A, %w, %d, %b, %B, %m, %y, %Y, %H, %I, %p, %M, %S, %f, %z, %Z, %j, %U, %W, %c, %x, %X)
            prefixed with either now or utcnow (e.g. {now%H})
        :keyword paths: DEPRECATED. The legacy paths argument supported for legacy reasons.
        """
        self._args = args
        self._kwargs = kwargs

        self.__test_run_uid = str(uuid.uuid4())
        self.__is_first_variation = True
        self.__Current_variation = (
            dict_util.frozendict()
        )  # Dict where in each variation the current parameter set in placed
        self.__Last_variation = (
            dict_util.frozendict()
        )  # Dict where in each variation the current parameter set in placed
        self.__test_pipe, self.__gui_pipe = Pipe()  # Pipes for signal exchanging between test and GUI
        self._signals = flow_control.MySignals(self.__test_pipe)  # Signal handler: test <--> GUI
        # Events. Used internally to wait for events like entered_pause, left_pause, etc.
        self._events = flow_control.Events()
        self._exec_ctrl = flow_control.FlowControl()
        self._executiondate = arrow.now()
        self._executiondate_utc = arrow.utcnow()
        self._inside_variation = False
        self._is_interactive_test = False
        self._test_started = False  # Flag to signalize that the startup is done
        self._shutdown_called = False  # Flag to signalize that the shutdown method was called
        self._error_traceback = None  # Stores a potential error traceback
        self._error_class = None  # Stores the exception class in cause an exception occurs during the test
        self._write_progress = True  # Writing of progress (x of nth variation) can be disabled for interactive testing
        self._iteration_dirnames = set()  # List of names of the directories of the testresults
        self._variation_index = 0  # The index of the current variation
        self._result_dir_iteration = None  # Holds the iteration result directory

        # self._sysargs will be updated by the __parse_arguments method.
        self._sysargs = {
            "result_dir": None,  # Working directory of the test. The test.log file will be stored there
            "logginglevel": logging.INFO,
            "teehouse_context": None,
        }

        self.__Attributes = Attributes(self.__class__.__name__)
        self.__RunOptions = RunOptions(self)
        self.__StorageOptions = StorageOptions(self)
        self.__Parameters: dict[str, Parameter] | Parameters = Parameters(self)
        self.__SweepManager = SweepManager()
        self.__Outputs: dict[str, Output] | Outputs = Outputs(self)
        self.__Limits = Limits(self)
        self.__xHub = xHub(self)
        self.__Results = Results(self)  # Results class where test stores its results every variation.
        self._result_container = ResultContainer()
        self.__ResultSummary = ResultSummary()
        self.__postproc_mode_handler = postproc_mode_handler()
        self.__Utils = Utils(self)  # Contains helper classes for the test like log entries, breakpoints etc
        self.__hookcaller = HookCaller(obj=self, bp=self.Utils.SetBreakPoint)
        self._PVCC = ParameterValueChangeCallbacks(self)
        self._result_gateway = ResultGateway()
        self._spec_checker = SpecificationChecker()
        self.__Reporting = Reporting(self)
        self._teehouse_client: TEEHouseClient = None

        self._test_file_path = abspath(sys.modules[self.__module__].__file__)
        self.__logger = logging_util.get_test_logger()

        try:
            self.Attributes.ExecutionMode = Enums.ExecutionMode.Undef

            # the test is instantiated and the test interface description is returned on stdout
            self.__parse_arguments()
            self._config = {
                "run_test_kwargs": {},
                "result_dir": self._kwargs.get("result_dir"),
                "logginglevel": logging._checkLevel(env.LOG_LEVEL)
                if env.LOG_LEVEL is not None
                else self._sysargs["logginglevel"],
            }

            if self._sysargs["result_dir"]:
                self._config["result_dir"] = abspath(self._sysargs["result_dir"])

            logging_util.reset_loggers()
            logging_util.initialize_logger(loglevel=self._config["logginglevel"])
            logging_util.add_stream_handler(loglevel=self._config["logginglevel"])

            # Load all available result handlers
            self._result_gateway.load_handlers()
            self._result_gateway.update_handler_config(
                name="dlh5_handler",
                enabled=True,
            )

            # Check if WaveWatson legacy mode is active and print a warning once before the iteration loop,
            # where these warnings are filtered. By calling the simplest method from the lib that triggers the
            # warning, we don't have to re-implement the version dependent warning message.
            with warnings.catch_warnings(record=True) as warning_list:
                Waveform.Generation.DC(10, 10, 1).mirror()
                try:
                    legacy_warning = warning_list[0]
                    self.__logger.warning(str(legacy_warning.message))
                except IndexError:
                    pass  # this just means there was no warning because the legacy mode wasn't used.

            self.StorageOptions.ConfigureDLH5Handler(True)
            self.StorageOptions.ConfigureExcelHandler(True)

            self.__load_teehouse_context(self._sysargs["teehouse_context"])
            self._teehouse_client._connect()

            self._deprecated_support()
            self.Utils.SetBreakPoint("Before DoE.Init")
            self.__hookcaller.execute_hooks("DoE.Init", must_exist=True)
            self.Utils.SetBreakPoint("After DoE.Init")

            # Extract the module and test documentation
            module_doc = dedent((inspect.getdoc(inspect.getmodule(self.__class__)) or "").strip())
            class_doc = dedent((inspect.getdoc(self) or "").strip())
            if self.Attributes.Test_Description in (None, ""):
                doc = module_doc + "\n\n" + class_doc
                self.Attributes.Test_Description = doc
            else:
                self.Attributes.Test_Description = dedent(self.Attributes.Test_Description.strip())

        except Exception as e:
            self._exec_ctrl.TestStatus = Enums.TestStatus.Error
            self._error_traceback = string_util.pretty_print_traceback(env.TRACEBACK_NR_FRAMES)
            self._error_class = e.__class__
            raise

    def __repr__(self):
        try:
            return str(self.Attributes.Test_Name)
        except Exception:
            return "Error when creating class representation string"

    def _deprecated_support(self):
        """
        Will be overwritten by descendant classes
        """
        # Create hooks for legacy test stages
        if hasattr(self, "Initialization"):
            self.Initialization = DoE.Init(prio=0)(self.Initialization)
        if hasattr(self, "PreLoop"):
            self.PreLoop = DoE.Startup(prio=0)(self.PreLoop)
        if hasattr(self, "VariationLoop"):
            self.VariationLoop = DoE.Iteration(prio=0)(self.VariationLoop)
        if hasattr(self, "VariationLoopPostprocessing"):
            self.VariationLoopPostprocessing = DoE.IterationPostprocessing(prio=0)(self.VariationLoopPostprocessing)
        if hasattr(self, "PostLoop"):
            self.PostLoop = DoE.Teardown(prio=0)(self.PostLoop)
        if hasattr(self, "OnError"):
            self.OnError = DoE.Error(prio=0)(self.OnError)

    def __parse_arguments(self):
        """
        The argument parser.
        """
        parser = argparse.ArgumentParser(description=self.__doc__)

        groupBasic = parser.add_argument_group("Options")
        groupBasic.add_argument(
            "-w",
            "-r",
            "--workingdir",
            "--result-dir",
            help="The working directory for the test output and controller logfiles.",
            type=str,
            default=None,
            required=False,
            dest="result_dir",
        )
        groupBasic.add_argument(
            "-c",
            "--teehouse_context",
            help="A json file containing the TEEHouse context",
            type=str,
            default=None,
            required=False,
        )
        groupBasic.add_argument(
            "-l",
            "--logginglevel",
            help="The logging level. DEBUG, INFO or ERROR.",
            default="INFO",
            required=False,
        )

        if "pytestrunner" not in sys.argv[0]:
            args = parser.parse_known_args()[0]

            self._sysargs["logginglevel"] = logging._checkLevel(args.logginglevel)
            self._sysargs["result_dir"] = args.result_dir
            self._sysargs["teehouse_context"] = args.teehouse_context

    def __load_teehouse_context(self, teehouse_context_file: str | None = None):
        self._teehouse_client = TEEHouseClient(
            ctx_file=teehouse_context_file,
            log=get_test_logger("TEEHouse Client: "),
            json_encoder_kwargs={"cls": json_encoder.CustomJSONEncoder},
        )
        if self._teehouse_client._mocked:
            return

        # Load optional configuration file
        ctx = self._teehouse_client.ctx
        self.__logger.info(f"Processing TEEHouse context file: {teehouse_context_file}")
        self.__logger.info(f"  TEEHouse Version: {ctx['static_ctx']['teehouse_version']}")

        self._config["result_dir"] = str(Path(ctx["runnable_ctx"]["workingdir_glob"]) / "data")
        self.__logger.info(f"  Using result directory: {self._config['result_dir']}")

    def __apply_teehouse_context(self):
        if self._teehouse_client._mocked:
            return

        ctx = self._teehouse_client.ctx
        for k, v in ctx["glob_iter_ctx"]["curr_parameters"].items():
            if k not in self.Parameters:
                self.__logger.info(f"  Adding new parameter {k!r} from global TEEHouse iteration context")
                self.Parameters.Add(Name=k, Unit="", Description="", DefaultValues=[v])
            else:
                self.__logger.info(
                    f"  Updating default value of parameter {k!r} from global TEEHouse iteration context"
                )
                self.Parameters[k].DefaultValues = [v]

                try:
                    self.__SweepManager.RemoveSweep(Param=k, all=True)
                    self.__logger.info(
                        f"  Removed all sweeps using parameter {k!r}, since it is forced as global "
                        f"iteration parameter by TEEHouse"
                    )
                except LookupError:
                    pass

        if len(ctx["runnable_ctx"].get("sweep_table", [])):
            self.__SweepManager.Clear()

        specs = ctx["runnable_ctx"]["specs"]
        if len(specs):
            self.Limits.clear()
            for spec in specs:
                self.Limits.Add(
                    spec["reference_output"],
                    spec["upper_threshold"],
                    spec["lower_threshold"],
                    spec["conditions"] or None,
                    spec["reference"] or None,
                    spec["x_range"],
                    spec["alias"],
                )

        runopt = ctx["runnable_ctx"]["run_options"]
        self.RunOptions.SuppressGUI = not runopt["show_gui"]
        self.RunOptions.PostProcOnlyReloadPath = runopt["post_processing_only_reload_path"]
        self.RunOptions.SkipPostProcessing = runopt["skip_iteration_postprocessing"]
        self.Attributes.ExecutionMode = {
            "lab": Enums.ExecutionMode.Lab,
            "sim": Enums.ExecutionMode.Sim,
            "simreload": Enums.ExecutionMode.Sim_Reload,
            "postproc": Enums.ExecutionMode.PostProcOnly,
        }.get(runopt["execution_mode"].lower(), Enums.ExecutionMode.Undef)

        # Set last variation parameters
        prev_params = ctx["glob_iter_ctx"]["prev_parameters"]
        orig_pnames = set(self.Parameters.keys())
        self.__Last_variation = {k: v for k, v in prev_params.items() if k in orig_pnames}

        for k, v in ctx["runnable_ctx"].get("metadata", {}).items():
            if not isinstance(v, (numbers.Number, bool, str)):
                self.__logger.warning(f"Ignoring metadata {k!r} from TEEHouse, since type of value {v!r} is invalid!")
                continue
            self.rc.add_global_user_metadata(k, v)

    def __get_test_interface(self):
        return {
            "attributes": dict(
                Testcase_Name=self.Attributes.Testcase_Name,
                Test_Name=self.Attributes.Test_Name,
                Test_Description=self.Attributes.Test_Description,
                **self.Attributes._user_defined_attributes,
            ),
            "parameters": self.Parameters,
            "outputs": self.Outputs,
            "limits": self.Limits,
            "sweeps": self.__SweepManager,
            "run_options": {
                "show_gui": not self.RunOptions.SuppressGUI,
                "post_processing_only_reload_path": self.RunOptions.PostProcOnlyReloadPath or "",
                "skip_iteration_postprocessing": self.RunOptions.SkipPostProcessing,
                "execution_mode": {
                    Enums.ExecutionMode.Lab: "lab",
                    Enums.ExecutionMode.Sim: "sim",
                    Enums.ExecutionMode.Sim_Reload: "simreload",
                    Enums.ExecutionMode.PostProcOnly: "postproc",
                    Enums.ExecutionMode.Undef: "lab",
                }.get(self.Attributes.ExecutionMode, "lab"),
            },
        }

    def RunTest(self, **kwargs):
        """
        Runs the test.
        """
        # Pot. passed run_test_kwargs has prio to directly supplied kwargs
        self._config["run_test_kwargs"] = {**kwargs, **self._config["run_test_kwargs"]}

        if self.Attributes.Testcase_Name is None:
            self.Attributes.Testcase_Name = self.Attributes.Test_Name  # Set Testcase Name to Testname is not specified

        # Commit any uncommited sweep definitions
        self.__SweepManager.CommitSweep()
        self.__SweepManager._init_sweep_table(self.Parameters)

        for key, value in (
            ("PyVerifyTestStarted", self._executiondate_utc.for_json()),
            ("PyVerifyTestClassName", self.Attributes.Test_Name),
            ("PyVerifyTestCaseName", self.Attributes.Testcase_Name),
            ("PyVerifyExecutionMode", self.Attributes.ExecutionMode),
            ("MeasurementName", self.Attributes.Test_Name),
            ("MeasurementID", "undefined"),
            ("GeneratorID", "PyVerify-" + __version__),
            ("Dirty", False),
            ("DirtyRationale", ""),
            ("Description", self.Attributes.Test_Description[:200]),
            ("MeasurementUUID", self.__test_run_uid),
        ):
            self._result_container._add_global_metadata(key, value)

        for key, value in (("Operator", getuser()), *tuple(self.Attributes._user_defined_attributes.items())):
            self._result_container.add_global_user_metadata(key, value)

        self.__logger.debug("Creating test interface.")
        if "interface_teehouse" in sys.argv:
            print("<INTERFACE>")
            print(json.dumps(self.__get_test_interface(), cls=json_encoder.CustomJSONEncoder))
            print("</INTERFACE>")
            sys.exit(0)

        self.__apply_teehouse_context()

        # Check if required information is missing
        if self.Attributes.ExecutionMode not in list(Enums.ExecutionMode.__dict__.values()):
            msg = (
                "Required attribute 'ExecutionMode' not specified.\nUse as follows:\n"
                "self.Attributes.ExecutionMode = Enums.ExecutionMode.Lab"
            )
            raise AttributeError(msg)

        with warnings.catch_warnings():
            # Finding a balance between showing the user relevant warnings but not spamming the log. Unfortunately the
            # filter action "once" doesn't work for warnings in the iteration loop for whatever reason. We are left with
            # "always" or "ignore" :/
            for w in (ResourceWarning,):
                warnings.simplefilter("ignore", w)
            # This particular warning is ignored to not spam the log. To compensate for this we will check for the
            # legacy mode during base test class init and present a warning once.
            warnings.filterwarnings(
                "ignore", message="The legacy default in-place operations of Waveform objects are deprecated"
            )

            if self.xHub._enabled:
                self._run_test_xhub(**kwargs)
            else:  # Normal operation
                self._run_test_normal(**kwargs)

            if self.TestStatus != Enums.TestStatus.Error:
                self._exec_ctrl.TestStatus = Enums.TestStatus.Done

            if not self.RunOptions.SuppressGUI:
                self._signals.test_terminated.emit()

        return self.TestStatus, self._error_class, self._error_traceback

    def _run_test_xhub(self, **kwargs):
        telemetry.track_feature(telemetry.FEATURES.pyverify_run_xhub)

        self.Variations.Clear()
        self._events.idle_state.clear()
        self._events.busy_state.set()
        self._exec_ctrl.TestStatus = Enums.TestStatus.Busy

        self._do_startup()
        if self._exec_ctrl.TestStatus != Enums.TestStatus.Error:
            self.xHub._register_actor()

            while True:
                if self._exec_ctrl.TestStatus == Enums.TestStatus.Error:
                    break

                try:
                    variation = self.xHub._pop_task_or_maybe_fetch()
                except (
                    self.xHub.xHubFetchTaskTimeoutError,
                    self.xHub.xHubControllerTerminated,
                ) as e:
                    self.Utils.LogInfo(str(e))
                    break
                except Exception as e:
                    self.Utils.LogError(f"An error occurred when fetching new tasks from xHub: {e}")
                    self._exec_ctrl.TestStatus = Enums.TestStatus.Error
                    break

                try:
                    cv = {}
                    cv.update(**{p[0]: p[1].DefaultValues[0] for p in self.Parameters.items()})
                    cv.update(**variation)
                    self.__Current_variation = dict_util.frozendict(cv)
                    if (
                        len(variation) == 2
                        and "DUT" in variation
                        and "Temp" in variation
                        and variation["DUT"] in ["None", None]
                        and variation["Temp"] == -273.15
                    ):
                        # Do nothing if no parameters specified
                        # (only standard parameters DUT and Temp have default values)
                        pass
                    else:
                        self._do_iteration()
                        self.xHub._post_results(statusMsg="Success")
                except Exception:
                    self.Utils.LogError(f"An unexpected exception occurred: {traceback.format_exc()}")
                    # self.xHub._post_results(statusMsg=f"Error({e})")
                    break

        if self._exec_ctrl.TestStatus != Enums.TestStatus.Error:
            self._do_teardown()
        else:
            self.__logger.info("Skipping execution of PostLoop, because test is in error state!")
        self.xHub._shutdown_controller()
        self.__shutdown()
        self._events.busy_state.clear()

    def _run_test_normal(self, **kwargs):
        telemetry.track_feature(telemetry.FEATURES.pyverify_run_bare)
        self._events.idle_state.clear()
        self._events.busy_state.set()
        self._exec_ctrl.TestStatus = Enums.TestStatus.Busy

        self._do_startup()

        if self._exec_ctrl.TestStatus != Enums.TestStatus.Error:
            while True:
                try:
                    if self._exec_ctrl.TestStatus == Enums.TestStatus.Error:
                        break
                    self.__logger.info("")
                    self.__logger.info("")

                    if not self._teehouse_client._mocked:
                        # Get global iteration parameters from TEEHouse context and merge into current variation
                        glob_param = self._teehouse_client.ctx["glob_iter_ctx"]["curr_parameters"]
                        self.__logger.info(f"Using global iteration parameters {glob_param} from TEEHouse context.")
                    else:
                        glob_param = {}

                    self.__Current_variation = dict_util.frozendict(
                        {**self.__SweepManager._sweep_table.current_row(unscale=False), **glob_param}
                    )

                    if (
                        len(self.__Current_variation) == 2
                        and "DUT" in self.__Current_variation
                        and "Temp" in self.__Current_variation
                        and self.__Current_variation["DUT"] in ["None", None]
                        and self.__Current_variation["Temp"] == -273.15
                    ):
                        pass  # Do nothing if no parameters specified (only standard parameters DUT
                        # and Temp have default values)
                    else:
                        self._do_iteration()
                        self.__SweepManager._sweep_table.next_row()
                except IndexError:
                    break

        if self._exec_ctrl.TestStatus != Enums.TestStatus.Error:
            self._do_teardown()
        else:
            self.__logger.info("Skipping execution of PostLoop, because test is in error state!")
        self.__shutdown()
        self._events.busy_state.clear()

    def RunUnittest(self):
        """
        Runs the test.
        :raises PyVerifyError: if the test executed with errors or specifications (limits) failed.
        """
        self.RunTest()
        if self.TestStatus == Enums.TestStatus.Error:
            msg = (
                "Test '{}' ended with errors. Please check logfile at {} " "and the logs in the run sub-directories!"
            ).format(self.Attributes.Testcase_Name, self.Utils.ResultDirectory / "test.log")
            raise Exception(msg)

        if any(
            [
                self.ResultSummary.Fail,
                self.ResultSummary.Unknown,
                self.ResultSummary.Error,
            ]
        ):
            raise Exception(
                "Test '%s' ended without errors, but %d specifications failed. "
                "Please check logfile at %s and the logs in the run sub-directories!"
                % (
                    self.Attributes.Testcase_Name,
                    self.ResultSummary.Fail,
                    self.Utils.ResultDirectory / "test.log",
                )
            )

    def _do_startup(self):
        __tracebackhide__ = True
        self.__t_test_started = time.perf_counter()
        self._test_started = True
        self._inside_variation = False
        try:
            self._variation_index = 0

            if self.Attributes.ExecutionMode not in list(Enums.ExecutionMode.__dict__.values()):
                msg = f"Unknown execution mode '{self.Attributes.ExecutionMode}'"
                raise AttributeError(msg)

            # Legacy support for 'paths' argument of TestBaseClass constructor
            if "paths" in self._kwargs and self._kwargs["paths"] is not None and self._config["result_dir"] is None:
                paths = self._kwargs["paths"]
                warnings.warn(
                    "The 'paths' keyword argument is deprecated. Use 'result_dir' instead.",
                    FutureWarning,
                    stacklevel=4,
                )
                if "DefaultResultDir" in paths:
                    self._config["result_dir"] = path_util.join_and_make_absolute(
                        paths["DefaultResultDir"],
                        self._executiondate.strftime("%Y-%m-%d_%H-%M-%S"),
                        self.Attributes.Testcase_Name,
                    )
                if "TestrunResult" in paths:
                    self._config["result_dir"] = path_util.join_and_make_absolute(
                        paths["TestrunResult"], "{testcasename}"
                    )
                if "TestResult" in paths:
                    self._config["result_dir"] = paths["TestResult"]

            # Create a workplace where the current test stores the variation results
            if not self._config["result_dir"]:
                # result_dir has not been set by legacy paths dict nor constructor keyword
                self._config["result_dir"] = path_util.join_and_make_absolute(
                    abspath(env.DEFAULT_RESULT_DIR),
                    self._executiondate.strftime("%Y-%m-%d_%H-%M-%S"),
                    self.Attributes.Testcase_Name,
                )
            else:
                now = arrow.now()
                utcnow = arrow.utcnow()
                format_args = {
                    "testname": self.Attributes.Test_Name,
                    "testcasename": self.Attributes.Testcase_Name,
                    "time": arrow.now().strftime("%H%M%S"),
                    "time_": arrow.now().strftime("%H_%M_%S"),
                    "date": arrow.now().strftime("%Y%m%d"),
                    "date_": arrow.now().strftime("%Y_%m_%d"),
                    "user": getuser().lower(),
                    "host": platform.node(),
                }
                for formatter in (
                    "%a",
                    "%A",
                    "%w",
                    "%d",
                    "%b",
                    "%B",
                    "%m",
                    "%y",
                    "%Y",
                    "%H",
                    "%I",
                    "%p",
                    "%M",
                    "%S",
                    "%f",
                    "%z",
                    "%Z",
                    "%j",
                    "%U",
                    "%W",
                    "%c",
                    "%x",
                    "%X",
                ):
                    format_args.update(
                        {
                            "now" + formatter: now.strftime(formatter),
                            "utcnow" + formatter: utcnow.strftime(formatter),
                        }
                    )
                self._config["result_dir"] = str(self._config["result_dir"]).format(**format_args)

            self.Utils.ResultDirectory.mkdir(parents=True, exist_ok=True)

            self._prepare_result_directory()
            (self.Utils.ResultDirectory / ".gitignore").write_text("*")

            self._result_gateway.update_handler_config(
                name=None,
                resultdir=self.Utils.ResultDirectory,
            )

            # ensure nextcloud client won't interfere with our workspace
            dlh5.api.check_for_nextcloud_monitoring(self.Utils.ResultDirectory)

            # Initialize all result handlers with the configured settings
            self._result_gateway.initialize_handlers()

            # Initialze logger for test
            logging_util.configure_test_logger(
                logdirectory=self.Utils.ResultDirectory,
                loglevel=self._config["logginglevel"],
            )

            self._teehouse_client._send_startup()

            if not self.RunOptions.SuppressGUI:
                # Start GUI
                self._events.gui_ready.clear()
                guicomm = Thread(
                    target=_gui_communicator,
                    name="gui_communicator",
                    args=(
                        self,
                        self.__test_pipe,
                        self.__gui_pipe,
                        self.Attributes.Test_Name,
                    ),
                    daemon=True,
                )
                guicomm.start()
                self.__logger.info("Waiting for GUI to start...")
                if not self._events.gui_ready.wait(env.GUI_START_TIMEOUT):
                    msg = "Could not start GUI"
                    raise Exception(msg)
                self.__logger.info("...GUI started")
                self._signals.test_started.emit()  # Emit test started signal -> TestController GUI is displayed

                logging_util.get_test_logger().addHandler(logging_util.GuiLogHandler(self._signals.display_log_msg))

            # Archive TEEHouse context file that is passed in
            if not self._teehouse_client._mocked:
                self.__logger.info(f"Archiving TEEHouse context {self._teehouse_client._ctx_file}")
                dst = self.Utils.ResultDirectory / "cfg"
                dst.mkdir(parents=True, exist_ok=True)
                shutil.copy(self._teehouse_client._ctx_file, dst)

            self.__logger.debug(f"ENVIRONMENT:\n{pprint.pformat(dict(os.environ))}")

            self.__logger.info("Command line: " + " ".join(sys.argv))
            self.__logger.info(f"Result Directory: {self.Utils.ResultDirectory}")
            self.__logger.info(f"Operator: {getuser()}")
            self.__logger.info(f"PID: {os.getpid()}")
            self.__logger.info(f"Python Version: {sys.version}")
            self.__logger.info(f"Python Executable: {Path(sys.executable)}")
            self.__logger.info("Python Path: " + ", ".join(sys.path))
            self.__logger.info(f"PyVerify Install Dir: {Path(__file__).parents[1]}")
            self.__logger.info(f"PyVerify Test Path: {Path(self._test_file_path)}")
            if "PYCHARM_HOSTED" in os.environ:
                self.__logger.info("IDE: PyCharm")

            self._result_gateway.add_static_data("test_documentation", self.Attributes.Test_Description)
            self._result_gateway.add_static_data("test_interface_definition", self.__get_test_interface())

            # GIT Info
            try:
                r_test = git.Repo(dirname(self._test_file_path), search_parent_directories=True)
                remote = r_test.remotes[0].name
                self.__logger.info("Git Remote Urls: " + ",".join(r_test.remote(remote).urls))
                self.__logger.info(f"Git Active Branch: {r_test.active_branch}")
                self.__logger.info(f"Git Commit: {r_test.active_branch.commit}")
                dirty = r_test.is_dirty()
                try:
                    remote_url = next(iter(r_test.remotes[0].urls))
                except Exception:
                    remote_url = "no-remote"
                try:
                    commit = str(r_test.active_branch.commit)
                except Exception:
                    commit = "unknown-commit"
                for key, value in (
                    ("MeasurementID", f"{remote_url};{commit}"),
                    ("Dirty", bool(dirty)),
                    ("DirtyRationale", "Repo of test dirty!" if dirty else ""),
                ):
                    self._result_container._add_global_metadata(key, value)
            except git.InvalidGitRepositoryError:
                pass
            except Exception as e:
                self.__logger.warning(f"Failed writing metadata about version control of test: {e}")

            if not self.Attributes.Testcase_Name or not isinstance(self.Attributes.Testcase_Name, str):
                self.__logger.warning(
                    "Testcase name is empty, None or is not a string. Using test name as testcase name"
                )
                self.Attributes.Testcase_Name = self.Attributes.Test_Name
                # raise AttributeError("Unvalid testcase name '%s'." % self.Attributes.Testcase_Name)

            # Results dict where test stores its results every variation. It is cleared before each new variation.
            self.__Results = Results(self)

            self.__logger.info(f"Executing test '{self.Attributes.Test_Name}'")

            # LAB: Initialize all instruments
            self.__logger.info(f"Execution mode: {self.Attributes.ExecutionMode}")
            if self.Attributes.ExecutionMode == Enums.ExecutionMode.Lab:
                telemetry.track_feature(telemetry.FEATURES.pyverify_mode_lab)
            elif self.Attributes.ExecutionMode in Enums.SimExecutionModes:
                telemetry.track_feature(telemetry.FEATURES.pyverify_mode_sim)
            elif self.Attributes.ExecutionMode == Enums.ExecutionMode.PostProcOnly:
                telemetry.track_feature(telemetry.FEATURES.pyverify_mode_pp)

            self.__log_test_description()

            if self.Attributes.ExecutionMode == Enums.ExecutionMode.PostProcOnly:
                # Get the path from where the results should be reloaded.
                # Reload the variation table of the loaded test and store it in
                self.RunOptions.PostProcOnlyReloadPath = self.__postproc_mode_handler.select_file(
                    self.RunOptions.PostProcOnlyReloadPath
                )
                self.__SweepManager._sweep_table.set_table(self.__postproc_mode_handler.get_test_variation_table())
            elif not self.xHub._enabled:
                if not self._teehouse_client._mocked:
                    # Overwrite sweep table from TEEHouse context. Sweeps were cleared beforehand.
                    sweeptable = self._teehouse_client.ctx["runnable_ctx"].get("sweep_table", [])
                    if len(sweeptable):
                        self.__logger.info("Overwriting sweep table from TEEHouse context")
                        self.__SweepManager._sweep_table.set_table(sweeptable)
                else:
                    self.__logger.info("Parameter defaults:")
                    for k, v in self.Parameters.items():
                        self.__logger.info(f"  {k}: {v.DefaultValues[0]!r}")

                self.__logger.info("")
                self.__logger.info(
                    "Sweep Definitions:\n" + textwrap.indent(self.__SweepManager._describe(), prefix="  ")
                )

                self._iteration_dirnames = {"run%04d" % x for x in range(len(self.__SweepManager._sweep_table))}
                self.__Current_variation = dict_util.frozendict(
                    self.__SweepManager._sweep_table.get_row(0)
                )  # Initialize self.Current_variation
                # with the first variation of the table
                self.__emit_new_variation()

            self._spec_checker.validate_configuration(
                parameter_defs=self.__Parameters, output_defs=self.__Outputs, spec_defs=self.__Limits
            )

        except BaseException:
            # If the test failed already here, do not execute the on_error hook
            self.__logger.info("Error in pre-user startup. Aborting...")
            self.__handle_error(execute_error_hook=False)
            return

        try:
            if self.Attributes.ExecutionMode != Enums.ExecutionMode.PostProcOnly:
                self.Utils.SetBreakPoint("Before DoE.Startup")
                self.__hookcaller.execute_hooks("DoE.Startup")
                self.Utils.SetBreakPoint("After DoE.Startup")
            else:
                self.__logger.info("Postprocessing-only mode: Skipping pre-loop")

            gc.collect()  # Call garbage collector to free ununsed memory
        except BaseException:
            self.__handle_error(execute_error_hook=True)

    def _prepare_result_directory(self):
        """
        Creates/cleans up the result directory
        """
        if self.Utils.ResultDirectory.exists():
            if (
                self.Attributes.ExecutionMode == Enums.ExecutionMode.Sim_Reload
                or len(self.RunOptions.RunVariations)
                or len(self.RunOptions.SkipVariations)
            ):
                files = [
                    self.Utils.ResultDirectory / f
                    for f in fnmatch_filter.filter(
                        names=os.listdir(self.Utils.ResultDirectory),
                        inclusion_patterns=[".*"],
                        exclusion_patterns=[r"run\d{4}", ".gitignore"],
                    )
                ]
                deleter.delete(files)

            elif self.Attributes.ExecutionMode != Enums.ExecutionMode.PostProcOnly:
                shutil.rmtree(self.Utils.ResultDirectory, ignore_errors=True)
                sleep(0.5)

        if not self.Utils.ResultDirectory.exists():
            # Create result directory
            for _i in range(10):
                try:  # Retry because of windoof filesystem
                    self.Utils.ResultDirectory.mkdir(parents=True, exist_ok=True)
                    break
                except PermissionError:
                    sleep(0.5)

    def _do_iteration(self):
        self.__logger.info("Begin new iteration...")
        __tracebackhide__ = True
        self.__t_iteration_started = time.perf_counter()

        _iteration_interrupted = False
        self._inside_variation = True
        try:
            if not self._shutdown_called:
                # Skip variation if
                if (
                    not self.xHub._enabled
                    and self.VariationIndex in self.RunOptions.SkipVariations
                    or (self.VariationIndex not in self.RunOptions.RunVariations and len(self.RunOptions.RunVariations))
                ):
                    self.__logger.info("Skipping variation %d" % self.VariationIndex)
                    self._variation_index += 1
                    return

                # Create directory for the current variation results

                self._result_dir_iteration = self.Utils.ResultDirectory / f"run{self._variation_index:04d}"
                if exists(self.Utils.ResultRunDirectory):
                    self.__logger.info(
                        f"Result directory {self.Utils.ResultRunDirectory!r} already exists. Cleaning up..."
                    )
                    if self.Attributes.ExecutionMode == Enums.ExecutionMode.Sim_Reload:
                        self.__logger.info(
                            "Removing all run result data except previous simulation data (simdata directory)"
                        )
                        for f in fnmatch_filter.filter(
                            names=os.listdir(self.Utils.ResultRunDirectory),
                            inclusion_patterns=[".*"],
                            exclusion_patterns=["simdata"],
                        ):
                            os.remove(join(self.Utils.ResultRunDirectory, f))
                    elif self.Attributes.ExecutionMode != Enums.ExecutionMode.PostProcOnly:
                        self.__logger.info(f"Removing all run result data at {self.Utils.ResultRunDirectory}")
                        shutil.rmtree(self.Utils.ResultRunDirectory, ignore_errors=True)
                    elif self.Attributes.ExecutionMode == Enums.ExecutionMode.PostProcOnly:
                        self.__logger.info("PostProcOnly mode: skipping cleanup")

                if not exists(self.Utils.ResultRunDirectory):
                    os.makedirs(self.Utils.ResultRunDirectory)
                    self.__logger.info(
                        "Creating result directory for variation %d at '%s'"
                        % (self._variation_index, self.Utils.ResultRunDirectory)
                    )

                self._iteration_dirnames.add(basename(self.Utils.ResultRunDirectory))

                self.__logger.info("Variation number: %d" % self.VariationIndex)
                self.__logger.info(
                    "Variation parameters: \n" + dict_util.pretty_print_dict(self.Current_variation, " " * 27, 1)
                )

                if self.Attributes.ExecutionMode == Enums.ExecutionMode.PostProcOnly:
                    # Reload json variation results in self.Results object
                    self.__logger.info("Postprocessing-only mode: Skipping variation loop")
                    # Reload the test results and overwrite the entries in self.Results (self.Results)
                    reloaded_results = self.__postproc_mode_handler.get_test_variation_results(self.VariationIndex)
                    for name, (data, metadata) in reloaded_results.items():
                        self._result_container.add(name, data, metadata)

                # Check parameter iteration values against potential thresholds
                # Although this is already done in the variation definitions (to fail early) it has to be redone here
                # since we can dynamically get parameter iterations through xHub
                for pname, pobj in self.Parameters.items():
                    if pobj.Max is not None and self.Current_variation[pname] > pobj.Max:
                        msg = (
                            f"Current iteration value of parameter {pname!r} exceeds its defined maximum! "
                            f"({self.Current_variation[pname]} > {pobj.Max})"
                        )
                        raise ValueError(msg)
                    if pobj.Min is not None and self.Current_variation[pname] < pobj.Min:
                        msg = (
                            f"Current iteration value of parameter {pname!r} exceeds its defined minimum! "
                            f"({self.Current_variation[pname]} < {pobj.Min})"
                        )
                        raise ValueError(msg)

                # Create a list of lists [<Parametername>[<Unit>], <CurrentValue>] to send to the GUI to display
                if not self.RunOptions.SuppressGUI:
                    self.__emit_new_variation()

                self._result_gateway.new_iteration(self.VariationIndex)

                if not self._teehouse_client._mocked:
                    self._teehouse_client._send_iteration_started(
                        n_iteration=self.VariationIndex,
                        total=len(self.__SweepManager._sweep_table),
                        parameters={
                            name: (value, (self.Parameters[name].Unit if name in self.Parameters else ""))
                            for name, value in self.Current_variation.items()
                        },
                    )

                # Skip VariationLoop if mode is PostProcOnly
                if self.Attributes.ExecutionMode != Enums.ExecutionMode.PostProcOnly:
                    self._result_container._add_iteration_metadata(
                        "MeasurementStartedTimestamp", arrow.utcnow().for_json()
                    )
                    cv_case_insensitive = dict_util.CaseInsensitiveDict(self.Current_variation)
                    for key in ("DUT", "Wafer", "Lot", "DesignStep", "ProductIdentifier"):
                        # see https://confluencewikiprod.intra.infineon.com/display/RDDL/Lab+Metadata
                        self._result_container.add_iteration_user_metadata(
                            key, cv_case_insensitive.get(key, "undefined")
                        )

                    self._PVCC.process_callbacks(Enums.InvokeTime.BeforeVarLoop)

                    self.Utils.SetBreakPoint("Before DoE.Iteration")
                    try:
                        self.__hookcaller.execute_hooks("DoE.Iteration", func_kwargs=self.Current_variation)
                    except IterationInterruptedError as e:
                        _iteration_interrupted = True
                        self.Utils.LogWarning(f"Current test iteration was interrupted: {e}")
                    self.Utils.SetBreakPoint("After DoE.Iteration")

                    if not _iteration_interrupted:
                        self._PVCC.process_callbacks(Enums.InvokeTime.AfterVarLoop)

                if not _iteration_interrupted:
                    if (
                        self.RunOptions.SkipPostProcessing
                        and self.Attributes.ExecutionMode != Enums.ExecutionMode.PostProcOnly
                    ):
                        self.__logger.info("Skipping variation loop postprocessing")
                    else:
                        self._PVCC.process_callbacks(Enums.InvokeTime.BeforeVarLoopPP)

                        self.Utils.SetBreakPoint("Before DoE.IterationPostprocessing")
                        try:
                            self.__hookcaller.execute_hooks(
                                "DoE.IterationPostprocessing",
                                func_kwargs=self.Current_variation,
                            )
                        except Exception:
                            self.__logger.warning(
                                "An error occurred during postprocessing of variation %d:\n%s"
                                % (
                                    self.VariationIndex,
                                    string_util.pretty_print_traceback(env.TRACEBACK_NR_FRAMES),
                                )
                            )
                        self.Utils.SetBreakPoint("After DoE.IterationPostprocessing")

                        self._PVCC.process_callbacks(Enums.InvokeTime.AfterVarLoopPP)

                        gc.collect()  # Call garbage collector to free unused memory

                # Log the results
                self.__logger.info("Variation results:")
                for res in sorted(self.Outputs.keys()):
                    if res in self._result_container._results:
                        data = self._result_container.get(res)
                        if isinstance(data, (tuple, list, ndarray)):
                            if len(data) > 20:
                                data = data[:5] + data[-5:]
                                data = [self.Outputs[res].format_value(val) for val in data]
                                resultstring = f"{data[:5]}, ..., {data[-5:]}]"
                            else:
                                resultstring = str([self.Outputs[res].format_value(val) for val in data])
                        else:
                            resultstring = self.Outputs[res].format_value(data)
                        self.__logger.info(f"  {res}: {resultstring} {self.Outputs[res].Unit}")
                    else:
                        self.__logger.info(f"  {res}: --- no value assigned ---")

                if self._write_progress:
                    total_iter = len(self.__SweepManager._sweep_table) if not self.xHub._enabled else "?"
                    self.__logger.info("Progress: %d/%s" % (self.VariationIndex + 1, total_iter))

                self.__post_iteration_results()

                if not _iteration_interrupted:
                    self.Utils.SetBreakPoint("Before DoE.PostExportResults")
                    self.__hookcaller.execute_hooks("DoE.PostExportResults", func_kwargs=self.Current_variation)
                    self.Utils.SetBreakPoint("After DoE.PostExportResults")

                self.__Last_variation = dict_util.frozendict(self.__Current_variation)
                self.__is_first_variation = False
                self._variation_index += 1

        except BaseException:
            self._variation_index += 1
            self.__Last_variation = dict_util.frozendict(self.__Current_variation)
            self.__is_first_variation = False
            self.__handle_error()

    def _do_teardown(self):
        __tracebackhide__ = True

        self._inside_variation = False
        try:
            self._teehouse_client._send_teardown()

            if self.Attributes.ExecutionMode != Enums.ExecutionMode.PostProcOnly:
                self.Utils.SetBreakPoint("Before DoE.Teardown")
                self.__hookcaller.execute_hooks("DoE.Teardown")
                self.Utils.SetBreakPoint("After DoE.Teardown")
            else:
                self.__logger.info("Postprocessing-only mode: Skipping post-loop")

        except BaseException:
            self.__handle_error()

    def __handle_error(self, context: str = "", execute_error_hook: bool = True):
        __tracebackhide__ = True

        etype, value, tb = sys.exc_info()
        if etype is not None:
            traceb = string_util.pretty_print_traceback(env.TRACEBACK_NR_FRAMES, context=context)
            self._error_class = str(etype.__name__)
            self._exec_ctrl.TestStatus = Enums.TestStatus.Error
            if etype is flow_control.SoftAbortRequestedError:
                self.__logger.error("Test aborted")
            elif isinstance(value, BaseException):
                self.__logger.error(traceb)

            if execute_error_hook:
                try:
                    self.__hookcaller.execute_hooks(
                        "DoE.Error", func_kwargs={"exc_info": (etype, value, tb)}, ignore_bp=True
                    )
                except Exception:
                    self.__handle_error(context="While executing hooks DoE.Error", execute_error_hook=False)
            self._error_traceback = traceb

            self._teehouse_client._send_failed(error_message=str(value), error_traceback=traceb)

    def __shutdown(self):
        self._inside_variation = False
        if self._shutdown_called:
            return
        self._shutdown_called = True

        if self.__logger is not None:  # Logger is None if self.__post_init() was not yet called and therefore
            try:
                self.__create_spec_summary()
            except Exception:
                self.__handle_error(context="While creating specification summary", execute_error_hook=False)

            try:
                self._result_gateway.add_static_data("error_traceback", self._error_traceback)
                self.__commit_result_file()
            except Exception:
                self.__handle_error(context="While committing the result file", execute_error_hook=False)

            self.__logger.info(f"Result directory:\n{self.Utils.ResultDirectory}")
            if self._config.get("forked_run", False):
                # When running the test forked, the parent process does not know where the test is storing its results
                # (no access to the actual object), since the result_dir configuration is evaluated during the test run.
                # So parent process captures result directory via tag on stdout
                sys.stdout.write(f"<result_directory>{self.Utils.ResultDirectory.as_posix()}</result_directory>")

            self.Utils.LogDebug("Removing empty result directories")
            for d in self.Utils.ResultDirectory.glob("*"):
                try:
                    if d.is_dir() and d.stat().st_size == 0:
                        d.rmdir()
                except Exception:
                    pass

            self._teehouse_client._disconnect()

            if self._exec_ctrl.TestStatus == Enums.TestStatus.Error:
                self.__logger.info(
                    "Done! [Exception(s) occurred, please examine the log entries above]"
                    "\n-------------------------------------------\n"
                )
            else:
                self.__logger.info("Done! [No Exceptions occurred]\n------------------------------------------\n")

            logging_util.reset_loggers()

            gc.collect()  # Call garbage collector to free ununsed memory
            if not self.RunOptions.SuppressGUI:
                self._signals.test_done.emit()

    def __post_iteration_results(self):
        self._result_container._validate_results(self.Outputs)

        thclient_payload = {
            "internal_iteration_index": self.VariationIndex,
            "parameters": {},
            "data": {},
            "metadata": {},
        }

        try:
            # Variations Inputs
            for p in self.Parameters.values():
                self._result_gateway.update_iteration_parameter(
                    p.Name,
                    self.Current_variation[p.Name],
                    p.Unit,
                )
                thclient_payload["parameters"][p.Name] = (self.Current_variation[p.Name], p.Unit)

            self.__logger.info("Evaluating specifications (spec-view):")
            spec: Specification
            for spec in self.Limits:
                result = self._spec_checker.check(
                    iteration_parameters=self.Current_variation,
                    iteration_results=self._result_container.get_all(),
                    specification=spec,
                )

                self._result_gateway.add_iteration_spec_eval_result(result=result)
                if self.xHub._enabled and spec.alias != "" and spec.conditions is None and spec.reference is None:
                    if result.overall_result in [
                        SpecificationResult.EvalResult.Pass,
                        SpecificationResult.EvalResult.Fail,
                    ]:
                        self.xHub._update_result(
                            spec.alias,
                            1 if result.overall_result == SpecificationResult.EvalResult.Pass else 0,
                        )
                    else:
                        self.__logger.warning(
                            f"Result of specification {spec.alias!r} is {result.overall_result.name!r}, "
                            f"which in invalid for use "
                            f"as xHub actor response. Treating this result as 'Failed'!"
                        )
                        self.xHub._update_result(spec.alias, 0)

                if result.overall_result == SpecificationResult.EvalResult.Error:
                    self.__logger.warning(f"  [ {result.overall_result.name:7s} ]  {spec}    [{result.error_msg}]")
                else:
                    self.__logger.info(f"  [ {result.overall_result.name:7s} ]  {spec}")
                    for rat in result.results_rationale:
                        if rat is not None:
                            self.__logger.info(f"               -> {rat}")

            self.__logger.info("Evaluating specifications (output-view):")
            # Variations Outputs
            for o in self.Outputs.values():
                applicable_results = [
                    spec_iteration_results[-1].overall_result
                    for spec_iteration_results in self._spec_checker.summary_result.values()
                    if spec_iteration_results[-1].specification.reference_output == o.Name
                ]

                consolidated_result = SpecificationResult._consolidate_eval_results(applicable_results)[0]
                self._result_container.update_result_metadata(o.Name, spec_status=consolidated_result.value)
                if consolidated_result != SpecificationResult.EvalResult.Unknown:
                    self.__logger.info(f"  [ {consolidated_result.value:7s} ]  {o.Name}")

                if self.xHub._enabled:
                    rvalue = self.rc.get(o.Name)
                    if numeric_util.is_numeric(rvalue):
                        if isnan(rvalue):
                            rvalue = "nan"
                        elif isinf(rvalue):
                            rvalue = "inf"
                        self.xHub._update_result(o.Name, rvalue)

                if o.Name in self._result_container._results:
                    res_meta = self._result_container._results_meta[o.Name]
                    res_meta["fmt"] = o.format
                    self._result_gateway.update_iteration_result(
                        o.Name,
                        self._result_container._results[o.Name],
                        o.Unit,
                        res_meta,
                    )
                    if o.dtype in (o.OutputType.Scalar, o.OutputType.String):
                        thclient_payload["data"][o.Name] = {
                            "data_type": "scalar",
                            "unit": o.Unit,
                            "data": self._result_container._results[o.Name],
                            "metadata": res_meta,
                        }
                    else:
                        pass  # todo: extend to array/waveform later
                else:  # No result available or None
                    self._result_gateway.update_iteration_result(
                        o.Name,
                        None,
                        o.Unit,
                        {"dtype": o.dtype, "spec_status": SpecificationResult.EvalResult.Unknown.value},
                    )

            self._result_container._add_iteration_metadata("PyVerifyIterationIndex", self.VariationIndex)
            self._result_container._add_iteration_metadata(
                "PyVerifyIterationDuration", time.perf_counter() - self.__t_iteration_started
            )

            for k, v in {
                **self._result_container._meta_iteration_user,
                **self._result_container._meta_iteration,
            }.items():
                self._result_gateway.update_iteration_metadata(k, v)
                thclient_payload["metadata"][k] = v

            self._result_gateway.commit_iteration()
            self._result_container._clear_iteration()

            if not self._teehouse_client._mocked:
                self._teehouse_client._send_iteration_finished(**thclient_payload)
        except Exception:
            self.__handle_error(context="While posting iteration results", execute_error_hook=False)

    def __create_spec_summary(self):
        try:
            self.__logger.info("")
            self.__logger.info("Specifications overview (specification-view,total):")
            self.__logger.info("  [ Overall (P %) | Pass   | Fail   | Skipped| Unknown| Error   ]")
            self.__logger.info("  [ ----------------------------------------------------------- ]")
            counter = Counter({"Pass": 0, "Fail": 0, "Skipped": 0, "Unknown": 0, "Error": 0})
            spec: Specification
            for spec in self.Limits:
                if spec.uid not in self._spec_checker.summary_result:
                    continue
                all_results = [
                    iteration_result.overall_result for iteration_result in self._spec_checker.summary_result[spec.uid]
                ]
                (
                    eval_result,
                    distrib,
                ) = SpecificationResult._consolidate_eval_results(all_results)
                # Counter each result to make an overview
                counter.update(distrib)
                distribution_fmt = (
                    f" {distrib['Pass']:<7d}| {distrib['Fail']:<7d}| {distrib['Skipped']:<7d}| "
                    f"{distrib['Unknown']:<7d}| {distrib['Error']:<7d}"
                )
                # Calc pass percentage
                passed = distrib["Pass"]
                failed = distrib["Fail"] + distrib["Unknown"] + distrib["Error"]
                try:
                    pass_percent = int(100.0 * passed / (passed + failed))
                except ZeroDivisionError:
                    pass_percent = 0
                if eval_result == SpecificationResult.EvalResult.Error:
                    for iteration_result in self._spec_checker.summary_result[spec.uid]:
                        if iteration_result.error_msg:
                            errMsg = iteration_result.error_msg
                            break
                    else:
                        errMsg = ""

                    self.__logger.warning(
                        f"  [ {eval_result.value:7s} {pass_percent:>3} % |{distribution_fmt} ]  {spec}    [{errMsg}]"
                    )
                else:
                    self.__logger.info(f"  [ {eval_result.value:7s} {pass_percent:>3} % |{distribution_fmt} ]  {spec}")

            self.__logger.info("  [ ----------------------------------------------------------- ]")
            passed = counter["Pass"]
            failed = counter["Fail"] + counter["Unknown"] + counter["Error"]
            try:
                pass_percent = int(100.0 * passed / (passed + failed))
            except ZeroDivisionError:
                pass_percent = 0
            self.__logger.info(
                f"  [ Total   {pass_percent:>3} % | "
                f"{counter['Pass']:<7d}| "
                f"{counter['Fail']:<7d}| "
                f"{counter['Skipped']:<7d}| "
                f"{counter['Unknown']:<7d}| "
                f"{counter['Error']:<7d} ]"
            )
            self.__logger.info("")

            self.__ResultSummary.update(
                {
                    "Pass": counter["Pass"],
                    "Fail": counter["Fail"],
                    "Skipped": counter["Skipped"],
                    "Unknown": counter["Unknown"],
                    "Error": counter["Error"],
                }
            )
        except Exception:
            self.__handle_error(context="While creating the specification summary", execute_error_hook=False)
            self.__ResultSummary.update({"Pass": 0, "Fail": 0, "Skipped": 0, "Unknown": 0, "Error": 0})

    def __commit_result_file(self):
        # Delete all run directories that were not used during this run but still exist due to
        # former test runs were the data was not deleted
        if self.Utils.ResultDirectory.exists():
            try:
                root, dirs, _ = next(os.walk(self.Utils.ResultDirectory, topdown=True))
                for name in dirs:
                    if name not in self._iteration_dirnames and re.match("run[0-9]+", name) is not None:
                        shutil.rmtree(join(root, name), ignore_errors=True)
            except Exception:
                self.__handle_error(context="Error occurred while deleting orphant data", execute_error_hook=False)

        try:
            self._result_container._add_global_metadata(
                "PyVerifyTestDuration", time.perf_counter() - self.__t_test_started
            )
            self._result_container._add_global_metadata("PyVerifySpecsPass", self.__ResultSummary["Pass"])
            self._result_container._add_global_metadata("PyVerifySpecsFail", self.__ResultSummary["Fail"])
            self._result_container._add_global_metadata("PyVerifySpecsSkipped", self.__ResultSummary["Skipped"])
            self._result_container._add_global_metadata("PyVerifySpecsUnknown", self.__ResultSummary["Unknown"])
            self._result_container._add_global_metadata("PyVerifySpecsError", self.__ResultSummary["Error"])

            for k in set(self._result_container._meta_global_user).intersection(
                set(self._result_container._meta_global)
            ):
                self.__logger.warning(f"User metadata {k!r} will be overwritten by eponymous system metadata!")

            for k, v in {**self._result_container._meta_global_user, **self._result_container._meta_global}.items():
                self._result_gateway.update_file_metadata(k, v)

            for h in self.__logger.handlers:
                if isinstance(h, logging.FileHandler):
                    with open(h.baseFilename) as fhLog:
                        self._result_gateway.add_static_data("test_log", fhLog.read())
                        break

            self._result_gateway.commit_file()
            self._result_container._clear_global()
        except Exception:
            self.__handle_error(context="While committing final test results", execute_error_hook=False)

    def __emit_new_variation(self):
        cv = []
        for p_name, p_val in self.Current_variation.items():
            if p_name in self.Parameters:
                unit = (
                    (f" [{ScalarUnit(self.Parameters[p_name].Unit).unscaled}]") if self.Parameters[p_name].Unit else ""
                )
                cv.append([p_name, str(p_val) + unit])
        self._signals.new_variation.emit(cv)  # Emit new variation signal to display the current parameters on the GUI

    def __log_test_description(self):
        """
        Will be called in the end of *_mtb_init*, the DoE.Init hook-wrapper and
        prints a description of the DoE into the log, e.g.

        ::

            ...  ---------------
            ...  DoE Description
            ...  ---------------
            ...    Initialization:
            ...      Hook-wrapper @DoE.Init on function 'MTB_Test._mtb_init'
            ...      Hook @DoE.Init on function 'TLE98xx_Test.init_parameters_outputs'
            ...      Hook @DoE.Init on function 'SpeedJumpTest.init'
            ...      Hook-wrapper @DoE.Init on function 'MTB_Test._mtb_init' (Teardown)
            ...    Startup:
            ...      Hook-wrapper @DoE.Startup on function 'PyVerifyBaseTest._at_doe_startup'
            ...      Hook-wrapper @DoE.Startup on function 'MTB_Test._mtb_startup'
            ...      Hook @DoE.Startup on function 'TLE98xx_Test.startup_dut'
            ...      Hook @DoE.Startup on function 'SpeedJumpTest.startup_scope'
            ...      Hook-wrapper @DoE.Startup on function 'MTB_Test._mtb_startup' (Teardown)
            ...      Hook-wrapper @DoE.Startup on function 'PyVerifyBaseTest._at_doe_startup' (Teardown)
            ...    Iteration:
            ...      Hook-wrapper @DoE.Iteration on function 'PyVerifyBaseTest._at_doe_iteration'
            ...      Hook-wrapper @DoE.Iteration on function 'MTB_Test._mtb_iteration'
            ...      Hook @DoE.Iteration on function 'SpeedJumpTest.trigger_setup'
            ...      Hook @DoE.Iteration on function 'SpeedJumpTest.plc_dut_comm_setup'
            ...      Hook @DoE.Iteration on function 'TLE98xx_Test.plc_trace_setup'
            ...      Hook @DoE.Iteration on function 'TLE98xx_Test.download_fw'
            ...      Hook-wrapper @DoE.Iteration on function 'MTB_Test._mtb_iteration' (Teardown)
            ...      Hook-wrapper @DoE.Iteration on function 'PyVerifyBaseTest._at_doe_iteration' (Teardown)
            ...    Iteration Postprocessing:
            ...      Hook-wrapper @DoE.IterationPostprocessing on function 'MTB_Test._mtb_postprocessing'
            ...      Hook @DoE.IterationPostprocessing on function 'TLE98xx_Test.data_postprocessing'
            ...      Hook @DoE.IterationPostprocessing on function 'SpeedJumpTest.filter_ts_waveform'
            ...      Hook @DoE.IterationPostprocessing on function 'SpeedJumpTest.strip_down_streamed_wf_to_scope'
            ...      Hook-wrapper @DoE.IterationPostprocessing on function 'MTB_Test._mtb_postprocessing' (Teardown)
            ...    Teardown:
            ...      Hook-wrapper @DoE.Teardown on function 'PyVerifyBaseTest._at_doe_teardown'
            ...      Hook-wrapper @DoE.Teardown on function 'MTB_Test._mtb_teardown'
            ...      Hook @DoE.Teardown on function 'TLE98xx_Test.teardown'
            ...      Hook-wrapper @DoE.Teardown on function 'MTB_Test._mtb_teardown' (Teardown)
            ...      Hook-wrapper @DoE.Teardown on function 'PyVerifyBaseTest._at_doe_teardown' (Teardown)
            ...  ---------------
            ...  ---------------
            ...  Test Parameters
            ...     DUT                            - The device under test (lab) or schematic (sim) used for the test.
            ...     Temp                           - [deg] The environment temperature used for the test.
            ...     DUT_chip_name                  - The chip name of the DUT (used to connect via debugger)
            ...     dut_app                        - A path to the FW ELF file used for debugging purposes
            ...     LowSpeed                       - A PLC test parameter
            ...     HighSpeed                      - A PLC test parameter
            ...
            ...  Test Outputs
            ...     U_L1                           - [V(s)] Voltage trace of phase 1 acquired by the scope
            ...     U_L2                           - [V(s)] Voltage trace of phase 2 acquired by the scope
            ...     U_L3                           - [V(s)] Voltage trace of phase 3 acquired by the scope
            ...     I_L1                           - [A(s)] Current trace of phase 1 acquired by the scope
            ...     I_L2                           - [A(s)] Current trace of phase 2 acquired by the scope
            ...     I_L3                           - [A(s)] Current trace of phase 3 acquired by the scope
            ...     U_bat                          - [V(s)] Trace of battery voltage acquired by the scope
            ...     I_bat                          - [A(s)] Trace of battery current acquired by the scope
            ...     RotorPos                       - [deg(s)] The rotor position
            ...     SpeedLoadTS                    - [rpm(s)] The motor speed
            ...     TorqueLoadTS                   - [Nm(s)] The motor torque
            ...  ---------------
        """
        log = self.Utils.LogInfo

        log("---------------")
        log("DoE Description")
        log("---------------")

        for msg, hookname in (
            ("Initialization", "DoE_Init"),
            ("Startup", "DoE_Startup"),
            ("Iteration", "DoE_Iteration"),
            ("Iteration Postprocessing", "DoE_IterationPostprocessing"),
            ("Teardown", "DoE_Teardown"),
            ("Error", "DoE_Error"),
        ):
            log(f"  {msg}:")
            hooks = self.HookCaller.get_sorted_hooks(hookname)
            for hook in hooks:
                if not hook.silent:
                    log(f"    {hook}")
            for hook in hooks[::-1]:
                if not hook.silent and hook.is_hook_wrapper:
                    log(f"    {hook} (Teardown)")

        log("---------------")
        log("---------------")
        log("Test Parameters")
        for p in self.Parameters.values():
            log(
                f"   {p.Name:<30} - "
                f"{ '[' + p.Unit + '] ' if p.Unit else ''}{p.Description if p.Description else 'no description'}"
            )
        log("")
        log("Test Outputs")
        for o in self.Outputs.values():
            log(
                f"   {o.Name:<30} - "
                f"{ '[' + o.Unit + '] ' if o.Unit else ''}{o.Description if o.Description else 'no description'}"
            )
        log("")
        log("Test Specifications")
        for limit in self.Limits:
            log(f"   {limit.describe(verbose=True)}")
        log("---------------")
        log("---------------")

    @DoE.Init(is_hook_wrapper=True, try_first=True, silent=True)
    def _at_doe_init(self):
        yield

    @DoE.Startup(is_hook_wrapper=True, try_first=True, silent=True)
    def _at_doe_startup(self):
        yield

    @DoE.Iteration(is_hook_wrapper=True, try_first=True, silent=True)
    def _at_doe_iteration(self):
        yield

    @DoE.Teardown(is_hook_wrapper=True, try_first=True, silent=True)
    def _at_doe_teardown(self):
        yield

    @DoE.Error(is_hook_wrapper=True, try_first=True, silent=True)
    def _at_doe_error(self):
        yield

    @property
    def TestStatus(self) -> str:
        """
        Returns a status string (one of Enums.TestStatus): "Idle", "Paused", "Busy", "Done", "Stopped", "Error"

        :rtype: str
        """
        return self._exec_ctrl.TestStatus

    @property
    def Attributes(self) -> Attributes:
        return self.__Attributes

    @property
    def RunOptions(self) -> RunOptions:
        return self.__RunOptions

    @property
    def Parameters(self) -> Parameters:
        return self.__Parameters

    @property
    def Outputs(self) -> dict[str, Output] | Outputs:
        return self.__Outputs

    @property
    def Limits(self) -> Limits:
        return self.__Limits

    @property
    def Variations(self) -> SweepManager:
        return self.__SweepManager

    @property
    def StorageOptions(self) -> StorageOptions:
        """Contains storage options for the test"""
        return self.__StorageOptions

    @property
    def Current_variation(self) -> dict[str, Any]:
        """
        Dictionary that holds the parameter values of the current variation.\n
        Use like this: self.Current_variation["<Parametername>"]
        """
        return self.__Current_variation

    @property
    def Current_variation_unscaled(self) -> dict[str, Any]:
        """
        Dictionary that holds the unscaled parameter values of the current variation.\n
        Use like this: self.Current_variation_unscaled["<Parametername>"]

        So if a parameter was specified with unit "mV" and value 10000, this dictionary will return the value as 10 V.
        """
        cv = self.Current_variation.copy()
        new_cv = {}
        for key, value in cv.items():
            if key in self.__Parameters:
                unit = ScalarUnit(self.__Parameters[key].Unit)
                unscaled_value, _ = unit.unscale(value)
                new_cv[key] = unscaled_value
            else:
                new_cv[key] = value
        return new_cv

    @property
    def Last_variation(self) -> dict[str, Any]:
        """
        Dictionary that holds the parameter values of the last variation.\n
        Use like this: self.Last_variation["<Parametername>"]

        :rtype: dict[str]
        """
        return self.__Last_variation

    @property
    def Results(self) -> Results:
        """
        Result Container.

        To assign data follow this notation:

        self.Result.<AnyOutputName> = <Value>

        <AnyOutputName> has to be specified as an output, otherwise it is ignored.

        <Value> has to fit to the defined datatype in the output definition.
        """
        return self.__Results

    @property
    def Utils(self) -> Utils:
        return self.__Utils

    @property
    def Reporting(self) -> Reporting:
        """
        Returns a subclass which includes reporting interface functions
        """
        return self.__Reporting

    @property
    def xHub(self) -> xHub:
        return self.__xHub

    @property
    def ResultSummary(self) -> ResultSummary:
        """
        Contains a summary about how many spec values have passed/failed or did not have limits.

        Use the attributes Pass/Fail/NoLim for checking.
        """
        return self.__ResultSummary

    @property
    def VariationIndex(self):
        """
        Returns the current variation index
        """
        return self._variation_index

    @property
    def HookCaller(self) -> HookCaller:
        return self.__hookcaller

    @property
    def rc(self) -> ResultContainer:
        """
        Returns a result container instance.
        Refer to pverify.core.internals.results.ResultContainer for more information.
        """
        return self._result_container

    @property
    def TeehouseClient(self):
        return self._teehouse_client

    @property
    def Postprocessings(self):
        warnings.warn(
            "As Avenue support has been removed, this property is deprecated "
            "and will be removed in a future version.",
            DeprecationWarning,
            stacklevel=2,
        )
        return unittest.mock.MagicMock()


def _gui_communicator(test_instance, test_pipe, gui_pipe, testname):
    log_test = logging_util.get_test_logger()
    p = None
    try:
        from multiprocessing import Process, Queue
        from multiprocessing.queues import Empty

        events = test_instance._events
        signals = test_instance._signals

        from pverify.gui.testcontroller.TestControllerGUI import testcontrollergui

        qu = Queue()

        p = Process(
            target=testcontrollergui,
            args=(
                qu,
                test_instance.Attributes.ExecutionMode,
                test_instance._config,
                test_instance._sysargs,
                gui_pipe,
                testname,
            ),
            name="TestControllerGui",
        )
        p.daemon = True
        p.start()

        exit = False
        while not exit:
            try:
                cmd = qu.get(timeout=0.5)
            except Empty:
                cmd = ""

            if not p.is_alive():
                log_test.error("Test GUI crashed! See log file.")
                os._exit(-1)

            if cmd in [
                "pause",
                "continue",
                "stop",
                "exit",
                "ready",
            ]:
                log_test.debug("Received command from GUI: " + cmd)

                if cmd == "pause":
                    events.entered_pause.clear()
                    test_instance._exec_ctrl.PauseRequest = True
                    if not events.entered_pause.wait(env.GUI_EVENT_TIMEOUT):
                        log_test.warning(
                            "Could not enter pause state within %ds. Resetting GUI control buttons."
                            % env.GUI_EVENT_TIMEOUT
                        )
                        test_instance._exec_ctrl.PauseRequest = False
                        signals.pause_left.emit()
                    else:
                        events.entered_pause.clear()

                elif cmd == "continue":
                    events.left_pause.clear()
                    test_instance._exec_ctrl.PauseRequest = False
                    if not events.left_pause.wait(env.GUI_EVENT_TIMEOUT):
                        log_test.warning(
                            "Could not leave pause state within %ds. Resetting GUI control buttons."
                            % env.GUI_EVENT_TIMEOUT
                        )
                        test_instance._exec_ctrl.PauseRequest = True
                        signals.pause_entered.emit()
                    else:
                        events.left_pause.clear()

                elif cmd == "stop":
                    # When GUI closes the GuiLogHandler has to be deleted immediatelly, otherwise it still sends
                    # signals to the GUI process and deadlocks the main application at some random time...
                    logger = logging_util.get_test_logger()
                    for hdlr in logger.handlers:
                        if isinstance(hdlr, logging_util.GuiLogHandler):
                            logger.removeHandler(hdlr)
                            logger.info("Removed GuiLogHandler from logging config")
                            break

                    test_instance._exec_ctrl.PauseRequest = False
                    test_instance._exec_ctrl.StopRequest = True
                    exit = True

                elif cmd == "ready":
                    events.gui_ready.set()

    except Exception as e:
        log_test.error(f"An error happened in GUI communicator: {e}", exc_info=True)

    finally:
        if p is not None and p.is_alive():
            # Kill process. If we don't do this some seconds after test ended a "Python crash window" will pop up.
            # Probably because the GUI process is somehow unproperly cleaned up.
            p.kill()
