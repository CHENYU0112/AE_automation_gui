from __future__ import annotations

import contextlib
import json
from pathlib import Path

import numpy as np
import pandas as pd

from pverify.core.io.hdfstore.pv_hdfstore import WFSTORE_NAME_FORMAT, PV_HDFStore

MIN_ITEMSIZE = 255


def dlh5_to_hdfstore(
    filepath_in: Path | str,
    filepath_out: Path | str | None = None,
    filepath_out_wf: Path | str | None = None,
    complevel=1,
    complib="blosc:zstd",
) -> Path:  # pragma: no cover
    """
    Converts DLH5 results to a HDFStore used by Pandas.

    Example::

         convert_to_pandas_hdfstore(
             filepath_in="C:\\temp\\DummyTest\\results.hdf5",
             filepath_out="C:\\temp\\DummyTest\\DummyTest_data.h5",
             filepath_out_wf="C:\\temp\\DummyTest\\DummyTest_wf.h5",
             complevel=9,
             complib="blosc"
         )

    :param filepath_in: The file path to the result data to be converted
    :param filepath_out: The target HDFStore file that contains the parameter indextable,
                         the datatable (only scalar results) and parameter info table
    :param filepath_out_wf: The target HDFStore file that contains the waveform results for all variations.
                            If None the filename is derived from the dst_data argument.
    :param complevel: int, 0-9, default None
                      Specifies a compression level for data.
                      A value of 0 disables compression.
    :param complib: {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'blosc'
                    Specifies the compression library to be used.

                    As of pandas v0.20.2 these additional compressors for Blosc are supported
                    (default if no compressor specified: 'blosc:blosclz'):
                    {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy', 'blosc:zlib', 'blosc:zstd'}.

                    Specifying a compression library which is not available issues
                    a ValueError.
    """
    from pverify.core.io.pv_dlh5 import PyVerifyDLH5 as DLH5

    filepath_in = Path(filepath_in)
    if filepath_out is None:
        filepath_out = filepath_in.with_suffix(".h5")
    filepath_out = Path(filepath_out)
    filepath_out.parent.mkdir(parents=True, exist_ok=True)
    if filepath_out_wf is None:
        filepath_out_wf = filepath_out.with_name(filepath_in.stem + "_wf.h5")

    with (
        pd.HDFStore(str(filepath_out), mode="w", complevel=complevel, complib=complib) as h5store,
        pd.HDFStore(str(filepath_out_wf), mode="w", complevel=complevel, complib=complib) as h5store_wf,
        DLH5(filepath_in, "r") as d,
    ):
        inputnames = d.get_index_operating_conditions().as_dataframe().columns.to_list()
        outputnames = []

        if not len(d.file_list_groups()):
            msg = "Cannot create PV_HDFStore because input hdf5 files don't contain variation results!"
            raise Exception(msg)

        # Create parameter/output info-table
        tinf = json.loads(d.static_get_text_data("test_interface_definition")[0])
        data = []
        for item in tinf["parameters"].values():
            data.append(
                (
                    PV_HDFStore.TYPE_PARAMETER,
                    item["Name"],
                    item["Unit"],
                    "string" if isinstance(item["DefaultValues"][0], (str, bytes)) else "numeric",
                    item["Description"],
                )
            )
        for item in tinf["outputs"].values():
            data.append((PV_HDFStore.TYPE_OUTPUT, item["Name"], item["Unit"], item["dtype"], item["Description"]))
        infotable = pd.DataFrame(data, None, ["type", "name", "unit", "dtype", "description"], str)
        h5store.put(PV_HDFStore.INFOTABLE, infotable, format="t", data_columns=True)

        grps = d.file_list_groups()
        var_cnt = len(grps)
        for i, gid in enumerate(grps):
            inputs = {k: v[0] for k, v in d.group_get_operating_conditions(gid, None).items()}
            scalar_outputs = {ch: d.group_get_channel(gid, ch)[0] for ch in d.list_numeric_channels(gid)}
            wf_outputs = {ch: d.group_get_channel(gid, ch)[0] for ch in d.list_waveform_channels(gid)}

            # Create indextable
            inputnames = list(set(inputnames + list(inputs.keys())))
            # Force convert int to float, otherwise different column types (int or float) for the same column
            # are used and pytables does not like that
            indextable = pd.DataFrame.from_dict({k: [float(v) if isinstance(v, int) else v] for k, v in inputs.items()})

            indextable.fillna(value=np.nan, inplace=True)
            object_cols = list(indextable.dtypes[indextable.dtypes == object].keys())
            # Truncate all strings to max MIN_ITEMSIZE
            indextable[object_cols] = indextable[object_cols].astype(str).map(lambda x: x[:MIN_ITEMSIZE])

            try:
                nrows = h5store.get_storer(PV_HDFStore.INDEXTABLE).nrows
            except Exception:
                nrows = 0
            indextable.index = pd.Series(indextable.index) + nrows  # Create unique index
            h5store.append(
                PV_HDFStore.INDEXTABLE,
                indextable,
                format="t",
                append=True,
                data_columns=True,
                index=False,
                expectedrows=var_cnt,
                min_itemsize=dict(zip(object_cols, [MIN_ITEMSIZE] * len(object_cols))),
            )

            # Create datatable
            outputnames = list(set(outputnames + list(scalar_outputs.keys()) + list(wf_outputs.keys())))
            datatable = pd.DataFrame(
                data=[list(scalar_outputs.values())],
                columns=list(scalar_outputs.keys()),
                dtype=np.float64,
            )
            datatable.fillna(value=np.nan, inplace=True)
            try:
                nrows = h5store.get_storer(PV_HDFStore.DATATABLE).nrows
            except Exception:
                nrows = 0
            datatable.index = pd.Series(datatable.index) + nrows  # Create unique index
            h5store.append(
                PV_HDFStore.DATATABLE,
                datatable,
                format="t",
                append=True,
                data_columns=True,
                index=False,
            )

            # Store waveforms
            wf_df = pd.DataFrame()
            for wname, wvalue in wf_outputs.items():
                if hasattr(wvalue, "time") and hasattr(wvalue, "data"):
                    wf_df = pd.merge(
                        wf_df,
                        pd.DataFrame(wvalue.data, wvalue.time, columns=[wname]),
                        how="outer",
                        left_index=True,
                        right_index=True,
                    )
            if not wf_df.empty:
                h5store_wf.put(WFSTORE_NAME_FORMAT % i, wf_df, format="f")

        h5store.create_table_index(PV_HDFStore.INDEXTABLE, columns=inputnames, optlevel=9, kind="full")
        with contextlib.suppress(KeyError):
            # This happens if the test did not produce scalar output data
            h5store.create_table_index(PV_HDFStore.DATATABLE, columns=outputnames, optlevel=9, kind="full")

    return filepath_out, filepath_out_wf
