# ruff: noqa: B023
from __future__ import annotations

from pathlib import Path

import junit_xml

from pverify.core.internals.specification import SpecificationResult
from pverify.core.util.dict_util import pretty_print_dict


def dlh5_to_junitxml(
    filepath_in: Path | str, filepath_out: Path | str | None = None, incl_parameter_info: bool = True
) -> Path:  # pragma: no cover
    """
    Creates a JUNIT xml file from DLH5. This only works with DLH5 stored by PyVerify, since PyVerify specific
    metadata about specifications are stored in string-typed channels.

    :param filepath_in: The file path to the result data to be converted
    :param filepath_out: The destination of the converted file.
                         If omitted it will be stored at the input file location with a different file suffix.
    :returns: The path to the converted file.
    """
    from pverify.core.io.pv_dlh5 import PyVerifyDLH5 as DLH5

    filepath_in = Path(filepath_in)
    if filepath_out is None:
        filepath_out = filepath_in.with_suffix(".xml")
    filepath_out = Path(filepath_out)
    filepath_out.parent.mkdir(parents=True, exist_ok=True)

    junitxml_ts = junit_xml.TestSuite(name="name_placeholder")
    with DLH5(filepath_in, "r") as d:
        junitxml_ts.name = junitxml_ts.package = d.file_get_metadata("PyVerifyTestCaseName")
        junitxml_ts.timestamp = d.file_get_metadata("PyVerifyTestStarted")
        junitxml_ts.hostname = d.file_get_metadata("host")
        junitxml_ts.stdout = d.static_get_text_data("test_log")

        for gid in d.file_list_groups():
            grp = d.file_get_group(gid)
            opcond = {k: " ".join(map(str, v)) for k, v in d.group_get_operating_conditions(grp, None).items()}
            for ch in d.list_string_channels(grp):
                ch_meta = d.group_get_channel_metadata(grp, ch)
                if ch_meta.get("subtype", "") != "specification_result":
                    continue
                result = ch_meta["specresult"]
                if result["overall_result"] == SpecificationResult.EvalResult.Skipped.value:
                    continue

                spec_id = result["specification"]["reference_output"]
                spec_alias = result["specification"]["alias"]
                spec_meta = {
                    "identifier": result["specification"]["reference_output"],
                    "alias": result["specification"]["alias"],
                    "conditions": result["specification"]["conditions"],
                    "reference": result["specification"]["reference"],
                    "upper_limit": result["specification"]["upper_thres"],
                    "lower_limit": result["specification"]["lower_thres"],
                    "x_range": result["specification"]["x_range"],
                }

                def format_stdout(description):
                    return f"""{description}

Identifier       : {spec_id!r}
Upper Limit      : {spec_meta["upper_limit"]!r}
Lower Limit      : {spec_meta["lower_limit"]!r}
Conditions       : {spec_meta["conditions"]!r}
x-Range          : {spec_meta["x_range"]!r}
Reference        : {spec_meta["reference"]!r}"""

                junit_tc = junit_xml.TestCase(
                    name=f"Iteration {gid:08d}",
                    classname=junitxml_ts.name + "." + spec_alias,
                )
                if result["overall_result"] == SpecificationResult.EvalResult.Pass.value:
                    junit_tc.stdout = format_stdout("All limits evaluated to either 'Pass' or 'Skipped'")
                elif result["overall_result"] == SpecificationResult.EvalResult.Fail.value:
                    kwargs = {
                        "output": ";".join(
                            [rat for rat in result.get("results_rationale", ["N.A."]) if rat is not None]
                        ),
                        "failure_type": "specification-failure",
                    }
                    junit_tc.add_failure_info(**kwargs)
                    junit_tc.stdout = format_stdout(kwargs["failure_type"] + ": " + kwargs["output"])
                elif result["overall_result"] == SpecificationResult.EvalResult.Unknown.value:
                    kwargs = {
                        "output": "Some or all checks has to be skipped due to missing/invalid data",
                        "error_type": "missing-data-error",
                    }
                    junit_tc.add_error_info(**kwargs)
                    junit_tc.stdout = format_stdout(kwargs["error_type"] + ": " + kwargs["output"])
                elif result["overall_result"] == SpecificationResult.EvalResult.Error.value:
                    error_msg = result.get("error_msg", "N.A.")
                    kwargs = {
                        "output": f"At least one exception occurred during evaluation! First error: {error_msg}",
                        "error_type": "evaluation-error",
                    }
                    junit_tc.add_error_info(**kwargs)
                    junit_tc.stdout = format_stdout(kwargs["error_type"] + ": " + kwargs["output"])
                else:
                    msg = f"Unknown result enum {result['overall_result']}!"
                    raise ValueError(msg)

                if incl_parameter_info:
                    junit_tc.stdout += "\n\nIteration Parameters:\n\n" + pretty_print_dict(opcond, " " * 4, 1)

                junitxml_ts.test_cases.append(junit_tc)

    with open(filepath_out, "w") as junitxml_fd:
        if hasattr(junit_xml, "to_xml_report_file"):
            junit_xml.to_xml_report_file(junitxml_fd, [junitxml_ts], encoding="utf-8")
        else:
            junit_xml.TestSuite.to_file(junitxml_fd, [junitxml_ts], encoding="utf-8")

    return filepath_out
